{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pipeline.driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "from kaggle_m5_nbdev.core import test_eq, test_err, configure_logging\n",
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG', date_format='XXXX-XX-XX XX:XX:XX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " INFO [XXXX-XX-XX XX:XX:XX] root: Not found parquet file (./tmp/processed-driver/sales_series_melt.parquet) - preparing the data\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Read 10 series\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Setting index\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Setting index - done\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding: id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding: item_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding: dept_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding: cat_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding: store_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding: state_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding: day_date_str\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding: date\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving encoder: id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving encoder: item_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving encoder: dept_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving encoder: cat_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving encoder: store_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving encoder: state_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving encoder: day_date_str\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving encoder: date\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving <class 'dask.dataframe.core.DataFrame'> to ./tmp/processed-driver/sales_series_melt.parquet\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding id (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root: 1707160 entries for id can't be labeled. Defaulting to FOODS_1_205_TX_1_validation e.g.\n",
      " ['HOBBIES_1_001_CA_1_validation' 'HOBBIES_1_002_CA_1_validation'\n",
      " 'HOBBIES_1_003_CA_1_validation']\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding date (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root: 1707440 entries for date can't be labeled. Defaulting to 2011-01-29 e.g.\n",
      " [datetime.date(2016, 6, 20) datetime.date(2016, 6, 20)\n",
      " datetime.date(2016, 6, 20)]\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding sell_price (float64) as float32 just in case for pytorch\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding cat_id (object) as categorical \n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding dept_id (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root: 604240 entries for dept_id can't be labeled. Defaulting to FOODS_1 e.g.\n",
      " ['HOBBIES_2' 'HOBBIES_2' 'HOBBIES_2']\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding item_id (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root: 1707440 entries for item_id can't be labeled. Defaulting to FOODS_1_205 e.g.\n",
      " ['HOBBIES_001' 'HOBBIES_002' 'HOBBIES_003']\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding state_id (object) as categorical \n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Encoding store_id (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root: 682976 entries for store_id can't be labeled. Defaulting to CA_1 e.g.\n",
      " ['CA_2' 'CA_2' 'CA_2']\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Setting index\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Setting index - done\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: Saving <class 'pandas.core.frame.DataFrame'> to ./tmp/processed-driver/test_series_melt.parquet\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: file:./tmp/processed-driver/sales_series_melt.parquet -> 1 files out of 4 remained after applying filter (.*part.(?!1).*)\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root: file:./tmp/processed-driver/sales_series_melt.parquet -> 1 files out of 4 remained after applying filter (.*part.1.*)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c7068086871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprepare_test_data_on_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_data_prep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opower/kaggle-m5-acc-nbdev/kaggle_m5_nbdev/pipeline/core.py\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(data, log, log_dir)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mlog_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"init\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_batch' is not defined"
     ]
    }
   ],
   "source": [
    "from kaggle_m5_nbdev.pipeline.core import prepare_data_on_disk, prepare_test_data_on_disk, setup_data_loaders\n",
    "from kaggle_m5_nbdev.pipeline.core import do_train\n",
    "\n",
    "processed_dir='./tmp/processed-driver'\n",
    "raw_dir='./raw'\n",
    "log_dir='./tmp/processed-driver/log'\n",
    "\n",
    "!mkdir -p {log_dir}\n",
    "!mkdir -p {processed_dir}\n",
    "\n",
    "n_sample_series=10\n",
    "prepare_data_on_disk(log, n_sample_series, processed_dir, raw_dir, force_data_prep=True)\n",
    "prepare_test_data_on_disk(log, raw_dir, processed_dir, force_data_prep=True)\n",
    "data = setup_data_loaders(processed_dir, log)\n",
    "do_train(data, log, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
