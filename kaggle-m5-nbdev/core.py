# AUTOGENERATED! DO NOT EDIT! File to edit: 00_core.ipynb (unless otherwise specified).

__all__ = ['test_eq', 'listify', 'test_in', 'test_err', 'configure_logging', 'setup_dataframe_copy_logging',
           'n_total_series', 'n_days_total', 'raw_dir', 'read_series_sample', 'melt_sales_series', 'extract_day_ids',
           'join_w_calendar', 'join_w_prices', 'to_parquet', 'extract_id_columns', 'get_submission_template_melt']

# Cell
from nbdev.showdoc import *
import pandas as pd
import logging
import datetime
import sys
import dask.dataframe as dd
import numpy as np
import traceback
from dask_ml import preprocessing as dask_preprocessing

# Cell
def test_eq(a,b): assert a==b, f'{a}, {b}'

from collections.abc import Sequence
def _seq_but_not_str(obj):
    return isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray))

def listify(obj):
    if _seq_but_not_str(obj):
        return obj

    return [obj]

def test_in(items, target):
    items = listify(items)
    missing = [item for item in items if item not in target]
    assert len(missing) == 0, f'{missing} are not in {target}'

def test_err(f, expected_message_part = None):
    try:
        f()
    except Exception as e:
        if not expected_message_part or expected_message_part in str(e):
            return
        else:
            raise ValueError(f"Expected different error to be thrown: {expected_message_part}")
    raise ValueError("Expected error to be thrown")

# Cell
def configure_logging(log_dir, log_name, log_lvl='DEBUG', con_log_lvl='INFO'):
    class IndentAdapter(logging.LoggerAdapter):
        def __init__(self, indent_start, indent_char, logger, extra):
            super().__init__(logger, extra)
            self.indent_start = indent_start
            self.indent_char = indent_char

        def indent(self):
            indentation_level = len(traceback.extract_stack())
            return indentation_level-self.indent_start-3 # indent + process + adapter call

        def process(self, msg, kwargs):
            return '{i}{m}'.format(i=self.indent_char*self.indent(), m=msg), kwargs

    log = logging.getLogger('root')
    already_initialized = any(filter(lambda h: isinstance(h, logging.StreamHandler), log.handlers))
    if already_initialized:
        print("Logging already initialized")
        return logging.getLogger('root')

    numeric_level = getattr(logging, log_lvl, None)
    log_format = '%(levelname)5s [%(asctime)s] %(name)s: %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    logging.basicConfig(
        filename=f'{log_dir}/{log_name}_{datetime.datetime.now().strftime("%Y-%m-%d_%H_%M_%S")}.txt',
        level=numeric_level,
        format=log_format,
        datefmt=date_format)
    log = logging.getLogger('root')
    ch = logging.StreamHandler()
    ch.setLevel(getattr(logging, con_log_lvl, None))
    ch.setFormatter(logging.Formatter(log_format, date_format))

    curr_indent = len(traceback.extract_stack())
    res = IndentAdapter(curr_indent, ' ', log, extra={})

    log.addHandler(ch)

    return res

# Cell
def setup_dataframe_copy_logging(log, threshold_mb):
    if not '_original_copy' in dir(pd.DataFrame):
        log.debug('Patching up DataFrame.copy')
        pd.DataFrame._original_copy = pd.DataFrame.copy
    else:
        log.debug('Patching up DataFrame.copy :: already done - skipping.')

    def _loud_copy(self, deep=True):
        size_mb = sys.getsizeof(self) / 1024 / 1024
        if size_mb >= threshold_mb:
            log.debug(f'Copying {size_mb:.1f} MiB (deep={deep})')

        return pd.DataFrame._original_copy(self, deep)

    pd.DataFrame.copy = _loud_copy

# Cell
n_total_series = 30490
n_days_total = 1913
raw_dir = 'raw'

# Cell
def read_series_sample(log, n):
    df = dd.read_csv(
        f'{raw_dir}/sales_train_validation.csv'
    ).sample(frac = n / n_total_series)
    log.debug(f"Read {len(df)} series")
    return df

# Cell
def melt_sales_series(df_sales_train):
    id_columns = [col for col in df_sales_train.columns if 'id' in col]
    sales_columns = [col for col in df_sales_train.columns if 'd_' in col]
    cat_columns = [col for col in id_columns if col != 'id']

    df_sales_train_melt = df_sales_train.melt(
        id_vars=id_columns,
        var_name='day_id',
        value_name='sales'
    )

    df_sales_train_melt['sales'] = df_sales_train_melt['sales'].astype('int16')

    return df_sales_train_melt

# Cell
def extract_day_ids(df_sales_train_melt):
    sales_columns = [f'd_{col}' for col in range(1, n_days_total+1)]
    mapping = {col: int(col.split('_')[1]) for col in sales_columns}
    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].map(mapping)

    import datetime
    d_1_date = pd.to_datetime('2011-01-29')
    mapping = {day:(d_1_date + datetime.timedelta(days=day-1)) for day in range(1, n_days_total+1)}
    df_sales_train_melt['day_date'] = df_sales_train_melt['day_id'].map(mapping)

    mapping = {day:str((d_1_date + datetime.timedelta(days=day-1)).date()) for day in range(1, n_days_total+1)}
    # gonna need it for joining with calendars & stuff
    df_sales_train_melt['day_date_str'] = df_sales_train_melt['day_id'].map(mapping)

    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].astype('int16')
    df_sales_train_melt['month_id'] = df_sales_train_melt['day_date'].dt.month.astype('uint8')

    return df_sales_train_melt

# Cell
def join_w_calendar(df_sales_train_melt, raw_dir):
    df_calendar = pd.read_csv(f'{raw_dir}/calendar.csv')

    df_calendar_melt = df_calendar.melt(
        id_vars=['date', 'wm_yr_wk', 'weekday', 'wday', 'year', 'd',
                'event_name_1', 'event_name_2', 'event_type_1', 'event_type_2'],
        value_name='snap_flag',
        var_name='state_id',
        value_vars=['snap_CA', 'snap_TX', 'snap_WI']
    )
    df_calendar_melt['snap_flag'] = df_calendar_melt['snap_flag'].astype('uint8')
    df_calendar_melt['state_id'] = df_calendar_melt['state_id'].str.split('_').str[1]

    df_sales_train_melt =  df_sales_train_melt.merge(
        df_calendar_melt[['date', 'state_id', 'wm_yr_wk', 'snap_flag']],
        left_on=['day_date_str', 'state_id'], right_on=['date', 'state_id'],
#  TODO: dask does not seem to support these       validate='many_to_one'
        )

    df_sales_train_melt['wm_yr_wk'] = df_sales_train_melt['wm_yr_wk'].astype('int16')
    return df_sales_train_melt

# Cell
def join_w_prices(partition, raw_dir):
    df_prices = pd.read_csv(f'{raw_dir}/sell_prices.csv')
    partition = partition.merge(
        df_prices,
        on=['store_id', 'item_id', 'wm_yr_wk'],
        how='left'
    )
    partition['sell_price'] = partition['sell_price'].astype('float32')
    partition['sales_dollars'] = (partition['sales'] * partition['sell_price']).astype('float32')
    partition = partition.fillna({'sales_dollars': 0}
    # TODO: doesn't seem to be supported by dask, inplace=True
    )
    return partition

# Cell
def to_parquet(sales_series, file_name, processed_dir, LOG):
    LOG.debug('Setting index')
    sales_series = sales_series.set_index(sales_series['id'])
    LOG.debug('Setting index - done')
    encoders = {}
    # TODO: dask supposedly does this on its own with sensible defaults
    # sales_series['parquet_partition'] = np.random.randint(0, 100, sales_series.shape[0])

    # this one is a dup of day_date_str which is harder to squeeze through the rest of the pipeline (yay petastorm)
    if 'day_date' in sales_series.columns:
        LOG.debug(f"Dropping 'day_date' from {sales_series.columns}")
        sales_series = sales_series.drop(['day_date'], axis=1)

    for col in sales_series.columns:
        if col in encoders:
            LOG.debug(f'Skipping: {col} - already encoded')
            continue

        # petastorm can't read these
        if str(sales_series[col].dtype) == 'uint8':
            sales_series[col] = sales_series[col].astype('int')

        if str(sales_series[col].dtype) in ['category', 'object']:
            LOG.debug(f'Encoding: {col}')
            enc = dask_preprocessing.LabelEncoder()
            #enc = LabelEncoder()
            sales_series[col] = enc.fit_transform(sales_series[col])
            # TODO: update other transforms too!
            encoders[col] = enc

    for name, enc in encoders.items():
        LOG.debug(f"Saving encoder: {name}")
        np.save(f'{processed_dir}/{name}.npy', enc.classes_)

    # TODO: uint -> int, category/object -> int, day_date -> drop
    parquet_file = f'{processed_dir}/{file_name}'
    LOG.debug(f"Saving to {parquet_file}")
    sales_series.to_parquet(
        parquet_file,
        # writing index blows up dask
        # also below keyword is dask, pandas would be: index=False,
        write_index=False,
#        partition_cols=['parquet_partition']
    )

# Cell
def extract_id_columns(t):
    extracted = t['id'].str.extract('([A-Z]+)_(\\d)_(\\d{3})_([A-Z]{2})_(\d)')
    t['cat_id'] = extracted[0]
    t['dept_id'] = t['cat_id'] + '_' + extracted[1]
    t['item_id'] = t['cat_id'] + '_' + extracted[2]
    t['state_id'] = extracted[3]
    t['store_id'] = t['state_id'] + '_' + extracted[4]
    return t

# Cell
from datetime import timedelta
def get_submission_template_melt(raw, d_1_date=pd.to_datetime('2016-06-20')):
    df_sample_submission = pd.read_csv(f'{raw}/sample_submission.csv')

    mapping = {f'F{day}':(d_1_date + timedelta(days=day)).date() for day in range(1,29)}
    mapping['id'] = 'id'
    df_sample_submission.columns = df_sample_submission.columns.map(mapping)
    df_sample_submission_melt = df_sample_submission.melt(id_vars='id', var_name='day', value_name='sales')

    last_prices = pd.read_csv(f'{raw}/sell_prices.csv')
    max_week = last_prices['wm_yr_wk'].max()
    last_prices = last_prices.query('wm_yr_wk == @max_week').copy()
    last_prices['id'] = last_prices['item_id'] + '_' + last_prices['store_id']
    last_prices_v = last_prices.copy()
    last_prices_e = last_prices

    last_prices_e['id'] = last_prices_e['id'] + '_evaluation'
    last_prices_v['id'] = last_prices_v['id'] + '_validation'
    last_prices = pd.concat([last_prices_e, last_prices_v], axis=0)[['id', 'sell_price']]

    df_sample_submission_melt = df_sample_submission_melt.merge(
        last_prices, on='id', how='left', validate='many_to_one')

    df_sample_submission_melt = extract_id_columns(df_sample_submission_melt)
    df_sample_submission_melt.drop(['sales'], axis=1, inplace=True)
    df_sample_submission_melt.rename({'day': 'date'}, axis=1, inplace=True)
    return df_sample_submission_melt