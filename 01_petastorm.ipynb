{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm import make_batch_reader, TransformSpec\n",
    "from petastorm.pytorch import DataLoader as PetaDataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader as TorchDataLoader, IterableDataset\n",
    "from torch import tensor\n",
    "from pyarrow.parquet import ParquetFile, ParquetReader\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def _init_filenames(log, filename_param, rex=None):\n",
    "    # note to self: I think that comes from petastorm/parquet but I don't really remember now :/\n",
    "    FILE_PREFIX = 'file:'\n",
    "    \n",
    "    if rex is None:\n",
    "        return filename_param\n",
    "\n",
    "    filename = filename_param[len(FILE_PREFIX):]\n",
    "    if not os.path.isdir(filename):\n",
    "        raise ValueError(f\"Filteri ng only possible for dirs, {filename} is not a one\")\n",
    "\n",
    "    paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(filename) for f in fn]\n",
    "    res = list(map(\n",
    "        lambda f: FILE_PREFIX + f,\n",
    "        filter(lambda f: re.match(rex, f) is not None, paths)\n",
    "    ))\n",
    "    if (len(res) == 0):\n",
    "        raise ValueError(f\"0 files remained out ot {len(paths)} - seems regex is too restrictive\")\n",
    "\n",
    "    if (len(res) == len(paths)):\n",
    "        raise ValueError(f\"{len(paths)} files remained out ot {len(paths)} - seems regex is a no op\")\n",
    "\n",
    "    log.debug(f\"{filename_param} -> {len(res)} files out of {len(paths)} remained after applying filter ({rex})\")\n",
    "    return res;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging already initialized\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# TODO: see if there are nicer ways of importing from other notebooks\n",
    "import sys\n",
    "sys.path.insert(1, './kaggle-m5-nbdev') \n",
    "from core import test_eq, test_err, configure_logging\n",
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tmp/_init_filenames_test',\n",
       " './tmp/_init_filenames_test/partition=0',\n",
       " './tmp/_init_filenames_test/partition=0/5846db13c7b744a5991598700f307860.parquet',\n",
       " './tmp/_init_filenames_test/partition=1',\n",
       " './tmp/_init_filenames_test/partition=1/b7b316e71f764762959315955ed5c26c.parquet']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = './tmp/_init_filenames_test'\n",
    "!rm -rf {test_dir}\n",
    "df = pd.DataFrame({\n",
    "    'id': np.arange(0, 1000), \n",
    "    'sell_price': np.arange(0, 1000)\n",
    "})\n",
    "df['partition'] = df['id'] % 2\n",
    "df.to_parquet(test_dir, partition_cols = ['partition'])\n",
    "files = !find {test_dir} | sort \n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-10 19:31:18] root: file:./tmp/_init_filenames_test -> 1 files out of 2 remained after applying filter (.*partition=1)\n"
     ]
    }
   ],
   "source": [
    "filename = f'file:{test_dir}'\n",
    "test_eq(filename, _init_filenames(log, filename))\n",
    "\n",
    "filtered_list = _init_filenames(log, filename, rex='.*partition=1')\n",
    "test_eq([f'file:{files[-1]}'], filtered_list)\n",
    "\n",
    "noop_filtered_list = lambda: _init_filenames(log, filename, rex='.*')\n",
    "test_err(noop_filtered_list, '2 files remained out ot 2')\n",
    "\n",
    "empty_filtered_list = lambda: _init_filenames(log, filename, rex='aa')\n",
    "test_err(empty_filtered_list, '0 files remained out ot 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetastormLeakyDescriptorsPatch():\n",
    "    def patch_leaking_fd(self, log):\n",
    "        def _patched_init(self, source, **kwargs):\n",
    "            self.source = source\n",
    "            return ParquetFile.__old_init__(self, source, **kwargs)\n",
    "\n",
    "        def _exit(self, *args, **kwargs):\n",
    "            if hasattr(self.source, 'close'):\n",
    "                self.source.close()\n",
    "                del self.source\n",
    "\n",
    "        def _bopen(fn):    \n",
    "            return open(fn, 'rb')\n",
    "\n",
    "        PetastormLeakyDescriptorsPatch.pre_open_fds = _bopen\n",
    "        if not hasattr(ParquetFile, '__old_init__'):\n",
    "            log.debug(\"Patching\")\n",
    "            ParquetFile.__old_init__ = ParquetFile.__init__\n",
    "\n",
    "            ParquetFile.__init__ = _patched_init\n",
    "            ParquetFile.__exit__ = _exit\n",
    "            ParquetFile.__del__ = _exit\n",
    "\n",
    "        else:\n",
    "            log.debug(\"Already patched\")\n",
    "            \n",
    "    def patch_loader(self, loader):\n",
    "        if PetastormLeakyDescriptorsPatch.pre_open_fds:\n",
    "            loader.reader.dataset.fs.open = pre_open_fds\n",
    "        else:\n",
    "            raise ValueError(\"Loader patching can't happen before class patching\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n",
      "  column_as_pandas = column.data.chunk(0).to_pandas()\n"
     ]
    }
   ],
   "source": [
    "loader = PetaDataLoader(\n",
    "    reader = make_batch_reader('file:./tmp/_init_filenames_test/',\n",
    "        schema_fields=['id'],\n",
    "        workers_count=1\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffling_queue_capacity=100000\n",
    ")\n",
    "# TODO: this not warning it out suggests the leak is no longer here?\n",
    "for batch in loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ParquetIterableDataset(IterableDataset):\n",
    "    def __init__(self, filename, log, rex=None):\n",
    "        super().__init__()\n",
    "        self._filename_param = filename\n",
    "        self.rex_param = rex\n",
    "        self.log = log\n",
    "        # self.patch = PetastormLeakyDescriptorsPatch()\n",
    "        # self.patch.patch_leaking_fd(log)\n",
    "        self.filename_param = filename\n",
    "        self.filename = _init_filenames(log, filename, rex)\n",
    "\n",
    "    def _init_petaloader(self):\n",
    "        def _transform_row(df_batch):\n",
    "            return df_batch\n",
    "\n",
    "        transform = TransformSpec(_transform_row, removed_fields=['cat_id', 'store_id', 'state_id'])\n",
    "        reader = make_batch_reader(self.filename,\n",
    "                 schema_fields=['id', 'item_id', 'dept_id', 'cat_id', 'day_id',\n",
    "               'sales', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
    "               'snap_flag', 'sell_price', 'sales_dollars', 'store_id', 'state_id'],\n",
    "                workers_count=1\n",
    "                #,transform_spec = transform\n",
    "        )\n",
    "        return PetaDataLoader(reader=reader, batch_size=128, shuffling_queue_capacity=100000)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1913*30490 # can be arbitrary large value to prevent WARN logs, seem to be ignored anyway\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.log.debug(f\"Iterator created on {self._filename_param}\")\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            count_cells = 0\n",
    "            count_batches = 0\n",
    "            with self._init_petaloader() as loader:\n",
    "                # self.patch.patch_loader(loader)\n",
    "                for batch in loader:\n",
    "                    count_batches += 1\n",
    "                    # TODO: propagate petaloader's batches without breaking them into individual items\n",
    "                    for idx in range(len(batch['sell_price'])):\n",
    "                        price         = batch['sell_price'][idx]\n",
    "                        sales_dollars = batch['sales_dollars'][idx] if ('sales_dollars' in batch) else -1.\n",
    "                        price_is_nan = math.isnan(price)\n",
    "                        # TODO: this starts to look like feature extraction, doesn't belong here\n",
    "                        price_or_zero = 0. if price_is_nan else price\n",
    "                        count_cells += 1\n",
    "                        # float32 needed for pytorch downstream\n",
    "                        yield {'features': tensor([price_or_zero, price_is_nan], dtype=torch.float32),\n",
    "                               'targets': tensor([sales_dollars])}\n",
    "                        \n",
    "            self.log.debug(f'Done iterating: {count_batches} batches / ({count_cells} cells) ')\n",
    "        else:\n",
    "            raise ValueError(\"Not implemented for multithreading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-10 19:31:22] root: Iterator created on file:./tmp/_init_filenames_test/\n",
      "DEBUG [2020-07-10 19:31:23] root: Iterator created on file:./tmp/_init_filenames_test/\n",
      "DEBUG [2020-07-10 19:31:24] root: Done iterating: 8 batches / (1000 cells) \n"
     ]
    }
   ],
   "source": [
    "ds = ParquetIterableDataset('file:./tmp/_init_filenames_test/', log)\n",
    "row = next(iter(ds))\n",
    "test_eq(tensor([-1.]), row['targets'])\n",
    "\n",
    "ds = ParquetIterableDataset('file:./tmp/_init_filenames_test/', log)\n",
    "test_eq(1000, len([row for row in ds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
