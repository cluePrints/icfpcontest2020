{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module name here\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from dask_ml import preprocessing as dask_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_eq(a,b): assert a==b, f'{a}, {b}'\n",
    "    \n",
    "from collections.abc import Sequence \n",
    "def _seq_but_not_str(obj):\n",
    "    return isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray))\n",
    "\n",
    "def listify(obj):\n",
    "    if _seq_but_not_str(obj):\n",
    "        return obj\n",
    "\n",
    "    return [obj]\n",
    "    \n",
    "def test_in(items, target):\n",
    "    items = listify(items)\n",
    "    missing = [item for item in items if item not in target]\n",
    "    assert len(missing) == 0, f'{missing} are not in {target}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in('a', ['a', 'b', 'c'])\n",
    "test_in(['b', 'c'], ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def configure_logging(log_dir, log_name, log_lvl='DEBUG', con_log_lvl='INFO'):\n",
    "    log = logging.getLogger('root')\n",
    "    already_initialized = any(filter(lambda h: isinstance(h, logging.StreamHandler), log.handlers))\n",
    "    if already_initialized:\n",
    "        print(\"Logging already initialized\")\n",
    "        return logging.getLogger('root')\n",
    "\n",
    "    numeric_level = getattr(logging, log_lvl, None)\n",
    "    log_format = '%(levelname)5s [%(asctime)s] %(name)s: %(message)s'\n",
    "    date_format = '%Y-%m-%d %H:%M:%S'\n",
    "    logging.basicConfig(\n",
    "        filename=f'{log_dir}/{log_name}_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")}.txt',\n",
    "        level=numeric_level,\n",
    "        format=log_format,\n",
    "        datefmt=date_format)\n",
    "    log = logging.getLogger('root')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(getattr(logging, con_log_lvl, None))\n",
    "    ch.setFormatter(logging.Formatter(log_format, date_format))\n",
    "    log.addHandler(ch)\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_dataframe_copy_logging(log, threshold_mb):\n",
    "    if not '_original_copy' in dir(pd.DataFrame):\n",
    "        log.debug('Patching up DataFrame.copy')\n",
    "        pd.DataFrame._original_copy = pd.DataFrame.copy\n",
    "    else:\n",
    "        log.debug('Patching up DataFrame.copy :: already done - skipping.')\n",
    "\n",
    "    def _loud_copy(self, deep=True):\n",
    "        size_mb = sys.getsizeof(self) / 1024 / 1024\n",
    "        if size_mb >= threshold_mb:\n",
    "            log.debug(f'Copying {size_mb:.1f} MiB (deep={deep})')\n",
    "\n",
    "        return pd.DataFrame._original_copy(self, deep)\n",
    "\n",
    "    pd.DataFrame.copy = _loud_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-06 11:33:26] root: Patching up DataFrame.copy\n"
     ]
    }
   ],
   "source": [
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG')\n",
    "setup_dataframe_copy_logging(log, threshold_mb=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a':[1,2,3]})\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "n_total_series = 30490\n",
    "n_days_total = 1913\n",
    "raw_dir = 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_series_sample(log, n):\n",
    "    df = dd.read_csv(\n",
    "        f'{raw_dir}/sales_train_validation.csv'\n",
    "    ).sample(frac = n / n_total_series)\n",
    "    log.debug(f\"Read {len(df)} series\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-06 11:33:32] root: Read 13 series\n"
     ]
    }
   ],
   "source": [
    "sample = read_series_sample(log, 13)\n",
    "test_eq(13, len(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def melt_sales_series(df_sales_train):\n",
    "    id_columns = [col for col in df_sales_train.columns if 'id' in col]\n",
    "    sales_columns = [col for col in df_sales_train.columns if 'd_' in col]\n",
    "    cat_columns = [col for col in id_columns if col != 'id']\n",
    "\n",
    "    df_sales_train_melt = df_sales_train.melt(\n",
    "        id_vars=id_columns,\n",
    "        var_name='day_id',\n",
    "        value_name='sales'\n",
    "    )\n",
    "\n",
    "    df_sales_train_melt['sales'] = df_sales_train_melt['sales'].astype('int16')\n",
    "\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_melt = melt_sales_series(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(n_days_total * 13, len(sample_melt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_day_ids(df_sales_train_melt):\n",
    "    sales_columns = [f'd_{col}' for col in range(1, n_days_total+1)]\n",
    "    mapping = {col: int(col.split('_')[1]) for col in sales_columns}\n",
    "    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    import datetime\n",
    "    d_1_date = pd.to_datetime('2011-01-29')\n",
    "    mapping = {day:(d_1_date + datetime.timedelta(days=day-1)) for day in range(1, n_days_total+1)}\n",
    "    df_sales_train_melt['day_date'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    mapping = {day:str((d_1_date + datetime.timedelta(days=day-1)).date()) for day in range(1, n_days_total+1)}\n",
    "    # gonna need it for joining with calendars & stuff\n",
    "    df_sales_train_melt['day_date_str'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].astype('int16')\n",
    "    df_sales_train_melt['month_id'] = df_sales_train_melt['day_date'].dt.month.astype('uint8')\n",
    "\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
       "       'sales', 'day_date', 'day_date_str', 'month_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt = extract_day_ids(sample_melt)\n",
    "sample_melt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(n_days_total * 13, len(sample_melt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_2_117_CA_1_validation</td>\n",
       "      <td>HOBBIES_2_117</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_2_117_CA_1_validation  HOBBIES_2_117  HOBBIES_2  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  day_id  sales   day_date day_date_str  month_id  \n",
       "0       CA       1      0 2011-01-29   2011-01-29         1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row = sample_melt.head(1)\n",
    "first_row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in(['day_date', 'day_date_str', 'day_id', 'month_id'], first_row.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('2011-01-29', first_row.loc[0, 'day_date_str'])\n",
    "test_eq(1,            first_row.loc[0, 'day_id'])\n",
    "test_eq(1,            first_row.loc[0, 'month_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_w_calendar(df_sales_train_melt):\n",
    "    df_calendar = pd.read_csv(f'{raw_dir}/calendar.csv')\n",
    "\n",
    "    df_calendar_melt = df_calendar.melt(\n",
    "        id_vars=['date', 'wm_yr_wk', 'weekday', 'wday', 'year', 'd',\n",
    "                'event_name_1', 'event_name_2', 'event_type_1', 'event_type_2'],\n",
    "        value_name='snap_flag',\n",
    "        var_name='state_id',\n",
    "        value_vars=['snap_CA', 'snap_TX', 'snap_WI']\n",
    "    )\n",
    "    df_calendar_melt['snap_flag'] = df_calendar_melt['snap_flag'].astype('uint8')\n",
    "    df_calendar_melt['state_id'] = df_calendar_melt['state_id'].str.split('_').str[1]\n",
    "\n",
    "    df_sales_train_melt =  df_sales_train_melt.merge(\n",
    "        df_calendar_melt[['date', 'state_id', 'wm_yr_wk', 'snap_flag']],\n",
    "        left_on=['day_date_str', 'state_id'], right_on=['date', 'state_id'],\n",
    "#  TODO: dask does not seem to support these       validate='many_to_one'\n",
    "        )\n",
    "\n",
    "    df_sales_train_melt['wm_yr_wk'] = df_sales_train_melt['wm_yr_wk'].astype('int16')\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_melt = join_w_calendar(sample_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>snap_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_2_117_CA_1_validation</td>\n",
       "      <td>HOBBIES_2_117</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_2_117_CA_1_validation  HOBBIES_2_117  HOBBIES_2  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  day_id  sales   day_date day_date_str  month_id        date  \\\n",
       "0       CA       1      0 2011-01-29   2011-01-29         1  2011-01-29   \n",
       "\n",
       "   wm_yr_wk  snap_flag  \n",
       "0     11101          0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test_not_in ('date') == dup of day_date_str\n",
    "test_in(['wm_yr_wk', 'snap_flag'], sample_melt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_w_prices(partition):\n",
    "    df_prices = pd.read_csv(f'{raw_dir}/sell_prices.csv')\n",
    "    partition = partition.merge(\n",
    "        df_prices,\n",
    "        on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "        how='left'\n",
    "    )\n",
    "    partition['sell_price'] = partition['sell_price'].astype('float32')\n",
    "    partition['sales_dollars'] = (partition['sales'] * partition['sell_price']).astype('float32')\n",
    "    partition = partition.fillna({'sales_dollars': 0}\n",
    "    # TODO: doesn't seem to be supported by dask, inplace=True\n",
    "    )\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-06 11:34:12] root: Copying 957.5 MiB (deep=True)\n",
      "DEBUG [2020-07-06 11:34:23] root: Copying 957.5 MiB (deep=True)\n"
     ]
    }
   ],
   "source": [
    "sample_melt = join_w_prices(sample_melt).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>snap_flag</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>sales_dollars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_2_117_CA_1_validation</td>\n",
       "      <td>HOBBIES_2_117</td>\n",
       "      <td>HOBBIES_2</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_2_117_CA_1_validation  HOBBIES_2_117  HOBBIES_2  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  day_id  sales   day_date day_date_str  month_id        date  \\\n",
       "0       CA       1      0 2011-01-29   2011-01-29         1  2011-01-29   \n",
       "\n",
       "   wm_yr_wk  snap_flag  sell_price  sales_dollars  \n",
       "0     11101          0         NaN            0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in(['sell_price', 'sales_dollars'], sample_melt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_parquet(sales_series, file_name, processed_dir, LOG):    \n",
    "    LOG.debug('Setting index')\n",
    "    sales_series = sales_series.set_index(sales_series['id'])\n",
    "    LOG.debug('Setting index - done')\n",
    "    encoders = {}\n",
    "    # TODO: dask supposedly does this on its own with sensible defaults\n",
    "    # sales_series['parquet_partition'] = np.random.randint(0, 100, sales_series.shape[0])\n",
    "\n",
    "    # this one is a dup of day_date_str which is harder to squeeze through the rest of the pipeline (yay petastorm)\n",
    "    if 'day_date' in sales_series.columns:\n",
    "        LOG.debug(f\"Dropping 'day_date' from {sales_series.columns}\")\n",
    "        sales_series = sales_series.drop(['day_date'], axis=1)\n",
    "\n",
    "    for col in sales_series.columns:\n",
    "        if col in encoders:\n",
    "            LOG.debug(f'Skipping: {col} - already encoded')\n",
    "            continue\n",
    "\n",
    "        # petastorm can't read these\n",
    "        if str(sales_series[col].dtype) == 'uint8':\n",
    "            sales_series[col] = sales_series[col].astype('int')\n",
    "\n",
    "        if str(sales_series[col].dtype) in ['category', 'object']:\n",
    "            LOG.debug(f'Encoding: {col}')            \n",
    "            enc = dask_preprocessing.LabelEncoder()\n",
    "            #enc = LabelEncoder()\n",
    "            sales_series[col] = enc.fit_transform(sales_series[col])\n",
    "            # TODO: update other transforms too!\n",
    "            encoders[col] = enc\n",
    "\n",
    "    for name, enc in encoders.items():\n",
    "        LOG.debug(f\"Saving encoder: {name}\")\n",
    "        np.save(f'{processed_dir}/{name}.npy', enc.classes_)\n",
    "\n",
    "    # TODO: uint -> int, category/object -> int, day_date -> drop\n",
    "    parquet_file = f'{processed_dir}/{file_name}'\n",
    "    LOG.debug(f\"Saving to {parquet_file}\")\n",
    "    sales_series.to_parquet(\n",
    "        parquet_file,\n",
    "        # writing index blows up dask\n",
    "        # also below keyword is dask, pandas would be: index=False,\n",
    "        write_index=False,\n",
    "#        partition_cols=['parquet_partition']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-06 11:52:53] root: Setting index\n",
      "DEBUG [2020-07-06 11:52:53] root: Setting index - done\n",
      "DEBUG [2020-07-06 11:52:53] root: Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [2020-07-06 11:52:53] root: Encoding: id\n",
      "DEBUG [2020-07-06 11:52:53] root: Encoding: item_id\n",
      "DEBUG [2020-07-06 11:52:54] root: Encoding: dept_id\n",
      "DEBUG [2020-07-06 11:52:56] root: Encoding: cat_id\n",
      "DEBUG [2020-07-06 11:52:58] root: Encoding: store_id\n",
      "DEBUG [2020-07-06 11:52:59] root: Encoding: state_id\n",
      "DEBUG [2020-07-06 11:53:02] root: Encoding: day_date_str\n",
      "DEBUG [2020-07-06 11:53:04] root: Encoding: date\n",
      "DEBUG [2020-07-06 11:53:08] root: Saving encoder: id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-06 11:53:08] root: Saving encoder: item_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-06 11:53:09] root: Saving encoder: dept_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-06 11:53:11] root: Saving encoder: cat_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-06 11:53:13] root: Saving encoder: store_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-06 11:53:15] root: Saving encoder: state_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-06 11:53:17] root: Saving encoder: day_date_str\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-06 11:53:19] root: Saving encoder: date\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-06 11:53:24] root: Saving to ./tmp/sample\n"
     ]
    }
   ],
   "source": [
    "to_parquet(sample_melt, 'sample', './tmp', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id:object',\n",
       " 'item_id:object',\n",
       " 'dept_id:object',\n",
       " 'cat_id:object',\n",
       " 'store_id:object',\n",
       " 'state_id:object',\n",
       " 'day_id:int16',\n",
       " 'sales:int16',\n",
       " 'day_date:datetime64[ns]',\n",
       " 'day_date_str:object',\n",
       " 'month_id:uint8',\n",
       " 'date:object',\n",
       " 'wm_yr_wk:int16',\n",
       " 'snap_flag:uint8',\n",
       " 'sell_price:float32',\n",
       " 'sales_dollars:float32']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: check if these can be read back well with a sibling func\n",
    "[f'{col}:{sample_melt[col].dtype}' for col in sample_melt.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOUSEHOLD_2_302_CA_4_validation    1913\n",
       "HOUSEHOLD_2_001_CA_3_validation    1913\n",
       "HOUSEHOLD_1_179_TX_1_validation    1913\n",
       "HOBBIES_2_117_CA_1_validation      1913\n",
       "FOODS_3_739_CA_3_validation        1913\n",
       "FOODS_3_431_WI_3_validation        1913\n",
       "FOODS_3_318_WI_1_validation        1913\n",
       "FOODS_3_119_TX_2_validation        1913\n",
       "FOODS_2_317_WI_1_validation        1913\n",
       "FOODS_2_095_TX_3_validation        1913\n",
       "FOODS_2_076_CA_1_validation        1913\n",
       "FOODS_1_053_CA_1_validation        1913\n",
       "FOODS_1_050_TX_2_validation        1913\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt['id'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
