{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module name here\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import traceback\n",
    "from dask_ml import preprocessing as dask_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_eq(a,b): assert a==b, f'{a}, {b}'\n",
    "    \n",
    "from collections.abc import Sequence \n",
    "def _seq_but_not_str(obj):\n",
    "    return isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray))\n",
    "\n",
    "def listify(obj):\n",
    "    if _seq_but_not_str(obj):\n",
    "        return obj\n",
    "\n",
    "    return [obj]\n",
    "    \n",
    "def test_in(items, target):\n",
    "    items = listify(items)\n",
    "    missing = [item for item in items if item not in target]\n",
    "    assert len(missing) == 0, f'{missing} are not in {target}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in('a', ['a', 'b', 'c'])\n",
    "test_in(['b', 'c'], ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def configure_logging(log_dir, log_name, log_lvl='DEBUG', con_log_lvl='INFO'):\n",
    "    class IndentAdapter(logging.LoggerAdapter):\n",
    "        def __init__(self, indent_start, indent_char, logger, extra):\n",
    "            super().__init__(logger, extra)\n",
    "            self.indent_start = indent_start\n",
    "            self.indent_char = indent_char\n",
    "\n",
    "        def indent(self):\n",
    "            indentation_level = len(traceback.extract_stack())\n",
    "            return indentation_level-self.indent_start-3 # indent + process + adapter call\n",
    "\n",
    "        def process(self, msg, kwargs):\n",
    "            return '{i}{m}'.format(i=self.indent_char*self.indent(), m=msg), kwargs\n",
    "\n",
    "    log = logging.getLogger('root')\n",
    "    already_initialized = any(filter(lambda h: isinstance(h, logging.StreamHandler), log.handlers))\n",
    "    if already_initialized:\n",
    "        print(\"Logging already initialized\")\n",
    "        return logging.getLogger('root')\n",
    "\n",
    "    numeric_level = getattr(logging, log_lvl, None)\n",
    "    log_format = '%(levelname)5s [%(asctime)s] %(name)s: %(message)s'\n",
    "    date_format = '%Y-%m-%d %H:%M:%S'\n",
    "    logging.basicConfig(\n",
    "        filename=f'{log_dir}/{log_name}_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")}.txt',\n",
    "        level=numeric_level,\n",
    "        format=log_format,\n",
    "        datefmt=date_format)\n",
    "    log = logging.getLogger('root')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(getattr(logging, con_log_lvl, None))\n",
    "    ch.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "    curr_indent = len(traceback.extract_stack())\n",
    "    res = IndentAdapter(curr_indent, ' ', log, extra={})\n",
    "    \n",
    "    log.addHandler(ch)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-08 19:37:29] root:  in test\n",
      "DEBUG [2020-07-08 19:37:29] root:   in test2\n",
      "DEBUG [2020-07-08 19:37:29] root: Interactive cell log\n"
     ]
    }
   ],
   "source": [
    "def _test():\n",
    "    log.debug('in test')\n",
    "    def _test2():\n",
    "        log.debug('in test2')\n",
    "    _test2()\n",
    "    \n",
    "_test()\n",
    "log.debug('Interactive cell log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_dataframe_copy_logging(log, threshold_mb):\n",
    "    if not '_original_copy' in dir(pd.DataFrame):\n",
    "        log.debug('Patching up DataFrame.copy')\n",
    "        pd.DataFrame._original_copy = pd.DataFrame.copy\n",
    "    else:\n",
    "        log.debug('Patching up DataFrame.copy :: already done - skipping.')\n",
    "\n",
    "    def _loud_copy(self, deep=True):\n",
    "        size_mb = sys.getsizeof(self) / 1024 / 1024\n",
    "        if size_mb >= threshold_mb:\n",
    "            log.debug(f'Copying {size_mb:.1f} MiB (deep={deep})')\n",
    "\n",
    "        return pd.DataFrame._original_copy(self, deep)\n",
    "\n",
    "    pd.DataFrame.copy = _loud_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-08 19:37:29] root:  Patching up DataFrame.copy\n"
     ]
    }
   ],
   "source": [
    "setup_dataframe_copy_logging(log, threshold_mb=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a':[1,2,3]})\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "n_total_series = 30490\n",
    "n_days_total = 1913\n",
    "raw_dir = 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_series_sample(log, n):\n",
    "    df = dd.read_csv(\n",
    "        f'{raw_dir}/sales_train_validation.csv'\n",
    "    ).sample(frac = n / n_total_series)\n",
    "    log.debug(f\"Read {len(df)} series\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-08 19:37:35] root:  Read 13 series\n"
     ]
    }
   ],
   "source": [
    "sample = read_series_sample(log, 13)\n",
    "test_eq(13, len(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def melt_sales_series(df_sales_train):\n",
    "    id_columns = [col for col in df_sales_train.columns if 'id' in col]\n",
    "    sales_columns = [col for col in df_sales_train.columns if 'd_' in col]\n",
    "    cat_columns = [col for col in id_columns if col != 'id']\n",
    "\n",
    "    df_sales_train_melt = df_sales_train.melt(\n",
    "        id_vars=id_columns,\n",
    "        var_name='day_id',\n",
    "        value_name='sales'\n",
    "    )\n",
    "\n",
    "    df_sales_train_melt['sales'] = df_sales_train_melt['sales'].astype('int16')\n",
    "\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_melt = melt_sales_series(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(n_days_total * 13, len(sample_melt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_day_ids(df_sales_train_melt):\n",
    "    sales_columns = [f'd_{col}' for col in range(1, n_days_total+1)]\n",
    "    mapping = {col: int(col.split('_')[1]) for col in sales_columns}\n",
    "    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    import datetime\n",
    "    d_1_date = pd.to_datetime('2011-01-29')\n",
    "    mapping = {day:(d_1_date + datetime.timedelta(days=day-1)) for day in range(1, n_days_total+1)}\n",
    "    df_sales_train_melt['day_date'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    mapping = {day:str((d_1_date + datetime.timedelta(days=day-1)).date()) for day in range(1, n_days_total+1)}\n",
    "    # gonna need it for joining with calendars & stuff\n",
    "    df_sales_train_melt['day_date_str'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].astype('int16')\n",
    "    df_sales_train_melt['month_id'] = df_sales_train_melt['day_date'].dt.month.astype('uint8')\n",
    "\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
       "       'sales', 'day_date', 'day_date_str', 'month_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt = extract_day_ids(sample_melt)\n",
    "sample_melt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(n_days_total * 13, len(sample_melt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_3_159_TX_1_validation</td>\n",
       "      <td>FOODS_3_159</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id      item_id  dept_id cat_id store_id state_id  \\\n",
       "0  FOODS_3_159_TX_1_validation  FOODS_3_159  FOODS_3  FOODS     TX_1       TX   \n",
       "\n",
       "   day_id  sales   day_date day_date_str  month_id  \n",
       "0       1      0 2011-01-29   2011-01-29         1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row = sample_melt.head(1)\n",
    "first_row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in(['day_date', 'day_date_str', 'day_id', 'month_id'], first_row.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('2011-01-29', first_row.loc[0, 'day_date_str'])\n",
    "test_eq(1,            first_row.loc[0, 'day_id'])\n",
    "test_eq(1,            first_row.loc[0, 'month_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_w_calendar(df_sales_train_melt):\n",
    "    df_calendar = pd.read_csv(f'{raw_dir}/calendar.csv')\n",
    "\n",
    "    df_calendar_melt = df_calendar.melt(\n",
    "        id_vars=['date', 'wm_yr_wk', 'weekday', 'wday', 'year', 'd',\n",
    "                'event_name_1', 'event_name_2', 'event_type_1', 'event_type_2'],\n",
    "        value_name='snap_flag',\n",
    "        var_name='state_id',\n",
    "        value_vars=['snap_CA', 'snap_TX', 'snap_WI']\n",
    "    )\n",
    "    df_calendar_melt['snap_flag'] = df_calendar_melt['snap_flag'].astype('uint8')\n",
    "    df_calendar_melt['state_id'] = df_calendar_melt['state_id'].str.split('_').str[1]\n",
    "\n",
    "    df_sales_train_melt =  df_sales_train_melt.merge(\n",
    "        df_calendar_melt[['date', 'state_id', 'wm_yr_wk', 'snap_flag']],\n",
    "        left_on=['day_date_str', 'state_id'], right_on=['date', 'state_id'],\n",
    "#  TODO: dask does not seem to support these       validate='many_to_one'\n",
    "        )\n",
    "\n",
    "    df_sales_train_melt['wm_yr_wk'] = df_sales_train_melt['wm_yr_wk'].astype('int16')\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_melt = join_w_calendar(sample_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>snap_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_3_159_TX_1_validation</td>\n",
       "      <td>FOODS_3_159</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id      item_id  dept_id cat_id store_id state_id  \\\n",
       "0  FOODS_3_159_TX_1_validation  FOODS_3_159  FOODS_3  FOODS     TX_1       TX   \n",
       "\n",
       "   day_id  sales   day_date day_date_str  month_id        date  wm_yr_wk  \\\n",
       "0       1      0 2011-01-29   2011-01-29         1  2011-01-29     11101   \n",
       "\n",
       "   snap_flag  \n",
       "0          0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test_not_in ('date') == dup of day_date_str\n",
    "test_in(['wm_yr_wk', 'snap_flag'], sample_melt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_w_prices(partition):\n",
    "    df_prices = pd.read_csv(f'{raw_dir}/sell_prices.csv')\n",
    "    partition = partition.merge(\n",
    "        df_prices,\n",
    "        on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "        how='left'\n",
    "    )\n",
    "    partition['sell_price'] = partition['sell_price'].astype('float32')\n",
    "    partition['sales_dollars'] = (partition['sales'] * partition['sell_price']).astype('float32')\n",
    "    partition = partition.fillna({'sales_dollars': 0}\n",
    "    # TODO: doesn't seem to be supported by dask, inplace=True\n",
    "    )\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-08 19:38:21] root: Copying 957.5 MiB (deep=True)\n",
      "DEBUG [2020-07-08 19:38:33] root: Copying 957.5 MiB (deep=True)\n"
     ]
    }
   ],
   "source": [
    "sample_melt = join_w_prices(sample_melt).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>snap_flag</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>sales_dollars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_3_159_TX_1_validation</td>\n",
       "      <td>FOODS_3_159</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>TX</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id      item_id  dept_id cat_id store_id state_id  \\\n",
       "0  FOODS_3_159_TX_1_validation  FOODS_3_159  FOODS_3  FOODS     TX_1       TX   \n",
       "\n",
       "   day_id  sales   day_date day_date_str  month_id        date  wm_yr_wk  \\\n",
       "0       1      0 2011-01-29   2011-01-29         1  2011-01-29     11101   \n",
       "\n",
       "   snap_flag  sell_price  sales_dollars  \n",
       "0          0         NaN            0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in(['sell_price', 'sales_dollars'], sample_melt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_parquet(sales_series, file_name, processed_dir, LOG):    \n",
    "    LOG.debug('Setting index')\n",
    "    sales_series = sales_series.set_index(sales_series['id'])\n",
    "    LOG.debug('Setting index - done')\n",
    "    encoders = {}\n",
    "    # TODO: dask supposedly does this on its own with sensible defaults\n",
    "    # sales_series['parquet_partition'] = np.random.randint(0, 100, sales_series.shape[0])\n",
    "\n",
    "    # this one is a dup of day_date_str which is harder to squeeze through the rest of the pipeline (yay petastorm)\n",
    "    if 'day_date' in sales_series.columns:\n",
    "        LOG.debug(f\"Dropping 'day_date' from {sales_series.columns}\")\n",
    "        sales_series = sales_series.drop(['day_date'], axis=1)\n",
    "\n",
    "    for col in sales_series.columns:\n",
    "        if col in encoders:\n",
    "            LOG.debug(f'Skipping: {col} - already encoded')\n",
    "            continue\n",
    "\n",
    "        # petastorm can't read these\n",
    "        if str(sales_series[col].dtype) == 'uint8':\n",
    "            sales_series[col] = sales_series[col].astype('int')\n",
    "\n",
    "        if str(sales_series[col].dtype) in ['category', 'object']:\n",
    "            LOG.debug(f'Encoding: {col}')            \n",
    "            enc = dask_preprocessing.LabelEncoder()\n",
    "            #enc = LabelEncoder()\n",
    "            sales_series[col] = enc.fit_transform(sales_series[col])\n",
    "            # TODO: update other transforms too!\n",
    "            encoders[col] = enc\n",
    "\n",
    "    for name, enc in encoders.items():\n",
    "        LOG.debug(f\"Saving encoder: {name}\")\n",
    "        np.save(f'{processed_dir}/{name}.npy', enc.classes_)\n",
    "\n",
    "    # TODO: uint -> int, category/object -> int, day_date -> drop\n",
    "    parquet_file = f'{processed_dir}/{file_name}'\n",
    "    LOG.debug(f\"Saving to {parquet_file}\")\n",
    "    sales_series.to_parquet(\n",
    "        parquet_file,\n",
    "        # writing index blows up dask\n",
    "        # also below keyword is dask, pandas would be: index=False,\n",
    "        write_index=False,\n",
    "#        partition_cols=['parquet_partition']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-08 19:38:35] root:  Setting index\n",
      "DEBUG [2020-07-08 19:38:36] root:  Setting index - done\n",
      "DEBUG [2020-07-08 19:38:36] root:  Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [2020-07-08 19:38:36] root:  Encoding: id\n",
      "DEBUG [2020-07-08 19:38:36] root:  Encoding: item_id\n",
      "DEBUG [2020-07-08 19:38:38] root:  Encoding: dept_id\n",
      "DEBUG [2020-07-08 19:38:39] root:  Encoding: cat_id\n",
      "DEBUG [2020-07-08 19:38:41] root:  Encoding: store_id\n",
      "DEBUG [2020-07-08 19:38:43] root:  Encoding: state_id\n",
      "DEBUG [2020-07-08 19:38:46] root:  Encoding: day_date_str\n",
      "DEBUG [2020-07-08 19:38:48] root:  Encoding: date\n",
      "DEBUG [2020-07-08 19:38:53] root:  Saving encoder: id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-08 19:38:54] root:  Saving encoder: item_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-08 19:38:55] root:  Saving encoder: dept_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-08 19:38:57] root:  Saving encoder: cat_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-08 19:38:59] root:  Saving encoder: store_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-08 19:39:01] root:  Saving encoder: state_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-08 19:39:03] root:  Saving encoder: day_date_str\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-08 19:39:06] root:  Saving encoder: date\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-08 19:39:10] root:  Saving to ./tmp/sample\n"
     ]
    }
   ],
   "source": [
    "to_parquet(sample_melt, 'sample', './tmp', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id:object',\n",
       " 'item_id:object',\n",
       " 'dept_id:object',\n",
       " 'cat_id:object',\n",
       " 'store_id:object',\n",
       " 'state_id:object',\n",
       " 'day_id:int16',\n",
       " 'sales:int16',\n",
       " 'day_date:datetime64[ns]',\n",
       " 'day_date_str:object',\n",
       " 'month_id:uint8',\n",
       " 'date:object',\n",
       " 'wm_yr_wk:int16',\n",
       " 'snap_flag:uint8',\n",
       " 'sell_price:float32',\n",
       " 'sales_dollars:float32']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: check if these can be read back well with a sibling func\n",
    "[f'{col}:{sample_melt[col].dtype}' for col in sample_melt.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOUSEHOLD_2_082_CA_3_validation    1913\n",
       "HOUSEHOLD_1_540_WI_2_validation    1913\n",
       "HOUSEHOLD_1_350_WI_1_validation    1913\n",
       "HOUSEHOLD_1_318_CA_3_validation    1913\n",
       "HOBBIES_2_048_WI_2_validation      1913\n",
       "HOBBIES_1_385_WI_3_validation      1913\n",
       "FOODS_3_634_TX_1_validation        1913\n",
       "FOODS_3_245_CA_1_validation        1913\n",
       "FOODS_3_229_WI_3_validation        1913\n",
       "FOODS_3_159_TX_1_validation        1913\n",
       "FOODS_2_120_TX_3_validation        1913\n",
       "FOODS_2_041_TX_1_validation        1913\n",
       "FOODS_1_050_TX_1_validation        1913\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt['id'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from petastorm import make_batch_reader, TransformSpec\n",
    "from petastorm.pytorch import DataLoader as PetaDataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader as TorchDataLoader, IterableDataset\n",
    "from torch import tensor\n",
    "from pyarrow.parquet import ParquetFile, ParquetReader\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import math\n",
    "\n",
    "FILE_PREFIX = 'file:'\n",
    "\n",
    "class PetastormLeakyDescriptorsPatch():\n",
    "    def patch_leaking_fd(self, log):\n",
    "        def _patched_init(self, source, **kwargs):\n",
    "            self.source = source\n",
    "            return ParquetFile.__old_init__(self, source, **kwargs)\n",
    "\n",
    "        def _exit(self, *args, **kwargs):\n",
    "            if hasattr(self.source, 'close'):\n",
    "                self.source.close()\n",
    "                del self.source\n",
    "\n",
    "        def _bopen(fn):    \n",
    "            return open(fn, 'rb')\n",
    "\n",
    "        PetastormLeakyDescriptorsPatch.pre_open_fds = _bopen\n",
    "        if not hasattr(ParquetFile, '__old_init__'):\n",
    "            log.debug(\"Patching\")\n",
    "            ParquetFile.__old_init__ = ParquetFile.__init__\n",
    "\n",
    "            ParquetFile.__init__ = _patched_init\n",
    "            ParquetFile.__exit__ = _exit\n",
    "            ParquetFile.__del__ = _exit\n",
    "\n",
    "        else:\n",
    "            log.debug(\"Already patched\")\n",
    "            \n",
    "    def patch_loader(self, loader):\n",
    "        if pre_open_fds:\n",
    "            loader.reader.dataset.fs.open = pre_open_fds\n",
    "        else:\n",
    "            raise ValueError(\"Loader patching can't happen before class patching\")\n",
    "\n",
    "class ParquetIterableDataset(IterableDataset):\n",
    "    def __init__(self, filename, log, rex=None):\n",
    "        super().__init__()\n",
    "        self._filename_param = filename\n",
    "        self.rex_param = rex\n",
    "        self.log = log\n",
    "        self.patch = PetastormLeakyDescriptorsPatch()\n",
    "        self.patch.patch_leaking_fd(log)\n",
    "        self.filename_param = filename\n",
    "        self.filename = self._init_filenames(filename, rex)\n",
    "\n",
    "    def _init_filenames(self, filename, rex):\n",
    "        if rex is None:\n",
    "            return filename\n",
    "        \n",
    "        filename = filename[len(FILE_PREFIX):]\n",
    "        if not os.path.isdir(filename):\n",
    "            raise ValueError(f\"Filtering only possible for dirs, {filename} is not a one\")\n",
    "\n",
    "        paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(filename) for f in fn]\n",
    "        res = list(map(\n",
    "            lambda f: FILE_PREFIX + f,\n",
    "            filter(lambda f: re.match(rex, f) is not None, paths)\n",
    "        ))\n",
    "        if (len(res) == 0):\n",
    "            raise ValueError(f\"0 files remained out ot {len(paths)} - seems regex is too restrictive\")\n",
    "\n",
    "        if (len(res) == len(paths)):\n",
    "            raise ValueError(f\"{len(paths)} files remained out ot {len(paths)} - seems regex is a no op\")\n",
    "\n",
    "        self.log.debug(f\"{self.filename_param} -> {len(res)} files out of {len(paths)} remained after applying filter ({self.rex_param})\")\n",
    "        return res;\n",
    "\n",
    "    def _init_petaloader(self):\n",
    "        def _transform_row(df_batch):\n",
    "            return df_batch\n",
    "\n",
    "        transform = TransformSpec(_transform_row, removed_fields=['cat_id', 'store_id', 'state_id'])\n",
    "        reader = make_batch_reader(self.filename,\n",
    "                 schema_fields=['id', 'item_id', 'dept_id', 'cat_id', 'day_id',\n",
    "               'sales', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
    "               'snap_flag', 'sell_price', 'sales_dollars', 'store_id', 'state_id'],\n",
    "                workers_count=1\n",
    "                #,transform_spec = transform\n",
    "        )\n",
    "        return PetaDataLoader(reader=reader, batch_size=128, shuffling_queue_capacity=100000)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1913*30490 # can be arbitrary large value to prevent WARN logs, seem to be ignored anyway\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.log.debug(f\"Iterator created on {self._filename_param}\")\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            count_cells = 0\n",
    "            count_batches = 0\n",
    "            with self._init_petaloader() as loader:\n",
    "                self.patch.patch_loader(loader)\n",
    "                for batch in loader:\n",
    "                    count_batches += 1\n",
    "                    # TODO: propagate petaloader's batches without breaking them into individual items\n",
    "                    for idx in range(len(batch['sell_price'])):\n",
    "                        price         = batch['sell_price'][idx]\n",
    "                        sales_dollars = batch['sales_dollars'][idx] if ('sales_dollars' in batch) else -1.\n",
    "                        price_is_nan = math.isnan(price)\n",
    "                        # TODO: this starts to look like feature extraction, doesn't belong here\n",
    "                        price_or_zero = 0. if price_is_nan else price\n",
    "                        count_cells += 1\n",
    "                        # float32 needed for pytorch downstream\n",
    "                        yield {'features': tensor([price_or_zero, price_is_nan], dtype=torch.float32),\n",
    "                               'targets': tensor([sales_dollars])}\n",
    "                        \n",
    "            self.log.debug(f'Done iterating: {count_batches} batches / ({count_cells} cells) ')\n",
    "        else:\n",
    "            raise ValueError(\"Not implemented for multithreading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-08 19:55:09] root:   Already patched\n",
      "DEBUG [2020-07-08 19:55:09] root:   file:./tmp/sample -> 1 files out of 4 remained after applying filter (.*part.(?!1).*)\n",
      "DEBUG [2020-07-08 19:55:10] root:  Iterator created on file:./tmp/sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'features': tensor([2.9800, 0.0000]), 'targets': tensor([0.])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ParquetIterableDataset(f'file:./tmp/sample', log, '.*part.(?!1).*')\n",
    "it = iter(ds)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
