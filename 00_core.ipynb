{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module name here\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import traceback\n",
    "from dask_ml import preprocessing as dask_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_eq(a,b): assert a==b, f'{a}, {b}'\n",
    "    \n",
    "from collections.abc import Sequence \n",
    "def _seq_but_not_str(obj):\n",
    "    return isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray))\n",
    "\n",
    "def listify(obj):\n",
    "    if _seq_but_not_str(obj):\n",
    "        return obj\n",
    "\n",
    "    return [obj]\n",
    "    \n",
    "def test_in(items, target):\n",
    "    items = listify(items)\n",
    "    missing = [item for item in items if item not in target]\n",
    "    assert len(missing) == 0, f'{missing} are not in {target}'\n",
    "    \n",
    "def test_err(f, expected_message_part = None):\n",
    "    try:\n",
    "        f()\n",
    "    except Exception as e:\n",
    "        if not expected_message_part or expected_message_part in str(e):\n",
    "            return\n",
    "        else:\n",
    "            raise ValueError(f\"Expected different error to be thrown: {expected_message_part}\")\n",
    "    raise ValueError(\"Expected error to be thrown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in('a', ['a', 'b', 'c'])\n",
    "test_in(['b', 'c'], ['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def configure_logging(log_dir, log_name, log_lvl='DEBUG', con_log_lvl='INFO'):\n",
    "    class IndentAdapter(logging.LoggerAdapter):\n",
    "        def __init__(self, indent_start, indent_char, logger, extra):\n",
    "            super().__init__(logger, extra)\n",
    "            self.indent_start = indent_start\n",
    "            self.indent_char = indent_char\n",
    "\n",
    "        def indent(self):\n",
    "            indentation_level = len(traceback.extract_stack())\n",
    "            return indentation_level-self.indent_start-3 # indent + process + adapter call\n",
    "\n",
    "        def process(self, msg, kwargs):\n",
    "            return '{i}{m}'.format(i=self.indent_char*self.indent(), m=msg), kwargs\n",
    "\n",
    "    log = logging.getLogger('root')\n",
    "    already_initialized = any(filter(lambda h: isinstance(h, logging.StreamHandler), log.handlers))\n",
    "    if already_initialized:\n",
    "        print(\"Logging already initialized\")\n",
    "        return logging.getLogger('root')\n",
    "\n",
    "    numeric_level = getattr(logging, log_lvl, None)\n",
    "    log_format = '%(levelname)5s [%(asctime)s] %(name)s: %(message)s'\n",
    "    date_format = '%Y-%m-%d %H:%M:%S'\n",
    "    logging.basicConfig(\n",
    "        filename=f'{log_dir}/{log_name}_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")}.txt',\n",
    "        level=numeric_level,\n",
    "        format=log_format,\n",
    "        datefmt=date_format)\n",
    "    log = logging.getLogger('root')\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(getattr(logging, con_log_lvl, None))\n",
    "    ch.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "    curr_indent = len(traceback.extract_stack())\n",
    "    res = IndentAdapter(curr_indent, ' ', log, extra={})\n",
    "    \n",
    "    log.addHandler(ch)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-15 05:57:24] root:  in test\n",
      "DEBUG [2020-07-15 05:57:24] root:   in test2\n",
      "DEBUG [2020-07-15 05:57:24] root: Interactive cell log\n"
     ]
    }
   ],
   "source": [
    "def _test():\n",
    "    log.debug('in test')\n",
    "    def _test2():\n",
    "        log.debug('in test2')\n",
    "    _test2()\n",
    "    \n",
    "_test()\n",
    "log.debug('Interactive cell log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup_dataframe_copy_logging(log, threshold_mb):\n",
    "    if not '_original_copy' in dir(pd.DataFrame):\n",
    "        log.debug('Patching up DataFrame.copy')\n",
    "        pd.DataFrame._original_copy = pd.DataFrame.copy\n",
    "    else:\n",
    "        log.debug('Patching up DataFrame.copy :: already done - skipping.')\n",
    "\n",
    "    def _loud_copy(self, deep=True):\n",
    "        size_mb = sys.getsizeof(self) / 1024 / 1024\n",
    "        if size_mb >= threshold_mb:\n",
    "            log.debug(f'Copying {size_mb:.1f} MiB (deep={deep})')\n",
    "\n",
    "        return pd.DataFrame._original_copy(self, deep)\n",
    "\n",
    "    pd.DataFrame.copy = _loud_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-15 05:57:24] root:  Patching up DataFrame.copy\n"
     ]
    }
   ],
   "source": [
    "setup_dataframe_copy_logging(log, threshold_mb=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a':[1,2,3]})\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "n_total_series = 30490\n",
    "n_days_total = 1913\n",
    "raw_dir = 'raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_series_sample(log, n):\n",
    "    df = dd.read_csv(\n",
    "        f'{raw_dir}/sales_train_validation.csv'\n",
    "    ).sample(frac = n / n_total_series)\n",
    "    log.debug(f\"Read {len(df)} series\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-15 05:57:30] root:  Read 13 series\n"
     ]
    }
   ],
   "source": [
    "sample = read_series_sample(log, 13)\n",
    "test_eq(13, len(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def melt_sales_series(df_sales_train):\n",
    "    id_columns = [col for col in df_sales_train.columns if 'id' in col]\n",
    "    sales_columns = [col for col in df_sales_train.columns if 'd_' in col]\n",
    "    cat_columns = [col for col in id_columns if col != 'id']\n",
    "\n",
    "    df_sales_train_melt = df_sales_train.melt(\n",
    "        id_vars=id_columns,\n",
    "        var_name='day_id',\n",
    "        value_name='sales'\n",
    "    )\n",
    "\n",
    "    df_sales_train_melt['sales'] = df_sales_train_melt['sales'].astype('int16')\n",
    "\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_melt = melt_sales_series(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(n_days_total * 13, len(sample_melt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_day_ids(df_sales_train_melt):\n",
    "    sales_columns = [f'd_{col}' for col in range(1, n_days_total+1)]\n",
    "    mapping = {col: int(col.split('_')[1]) for col in sales_columns}\n",
    "    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    import datetime\n",
    "    d_1_date = pd.to_datetime('2011-01-29')\n",
    "    mapping = {day:(d_1_date + datetime.timedelta(days=day-1)) for day in range(1, n_days_total+1)}\n",
    "    df_sales_train_melt['day_date'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    mapping = {day:str((d_1_date + datetime.timedelta(days=day-1)).date()) for day in range(1, n_days_total+1)}\n",
    "    # gonna need it for joining with calendars & stuff\n",
    "    df_sales_train_melt['day_date_str'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].astype('int16')\n",
    "    df_sales_train_melt['month_id'] = df_sales_train_melt['day_date'].dt.month.astype('uint8')\n",
    "\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
       "       'sales', 'day_date', 'day_date_str', 'month_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt = extract_day_ids(sample_melt)\n",
    "sample_melt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(n_days_total * 13, len(sample_melt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOUSEHOLD_1_365_CA_2_validation</td>\n",
       "      <td>HOUSEHOLD_1_365</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>CA_2</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id          item_id      dept_id     cat_id  \\\n",
       "0  HOUSEHOLD_1_365_CA_2_validation  HOUSEHOLD_1_365  HOUSEHOLD_1  HOUSEHOLD   \n",
       "\n",
       "  store_id state_id  day_id  sales   day_date day_date_str  month_id  \n",
       "0     CA_2       CA       1      5 2011-01-29   2011-01-29         1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row = sample_melt.head(1)\n",
    "first_row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in(['day_date', 'day_date_str', 'day_id', 'month_id'], first_row.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('2011-01-29', first_row.loc[0, 'day_date_str'])\n",
    "test_eq(1,            first_row.loc[0, 'day_id'])\n",
    "test_eq(1,            first_row.loc[0, 'month_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_w_calendar(df_sales_train_melt, raw_dir):\n",
    "    df_calendar = pd.read_csv(f'{raw_dir}/calendar.csv')\n",
    "\n",
    "    df_calendar_melt = df_calendar.melt(\n",
    "        id_vars=['date', 'wm_yr_wk', 'weekday', 'wday', 'year', 'd',\n",
    "                'event_name_1', 'event_name_2', 'event_type_1', 'event_type_2'],\n",
    "        value_name='snap_flag',\n",
    "        var_name='state_id',\n",
    "        value_vars=['snap_CA', 'snap_TX', 'snap_WI']\n",
    "    )\n",
    "    df_calendar_melt['snap_flag'] = df_calendar_melt['snap_flag'].astype('uint8')\n",
    "    df_calendar_melt['state_id'] = df_calendar_melt['state_id'].str.split('_').str[1]\n",
    "\n",
    "    df_sales_train_melt =  df_sales_train_melt.merge(\n",
    "        df_calendar_melt[['date', 'state_id', 'wm_yr_wk', 'snap_flag']],\n",
    "        left_on=['day_date_str', 'state_id'], right_on=['date', 'state_id'],\n",
    "#  TODO: dask does not seem to support these       validate='many_to_one'\n",
    "        )\n",
    "\n",
    "    df_sales_train_melt['wm_yr_wk'] = df_sales_train_melt['wm_yr_wk'].astype('int16')\n",
    "    return df_sales_train_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_melt = join_w_calendar(sample_melt, raw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>snap_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOUSEHOLD_1_365_CA_2_validation</td>\n",
       "      <td>HOUSEHOLD_1_365</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>CA_2</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id          item_id      dept_id     cat_id  \\\n",
       "0  HOUSEHOLD_1_365_CA_2_validation  HOUSEHOLD_1_365  HOUSEHOLD_1  HOUSEHOLD   \n",
       "\n",
       "  store_id state_id  day_id  sales   day_date day_date_str  month_id  \\\n",
       "0     CA_2       CA       1      5 2011-01-29   2011-01-29         1   \n",
       "\n",
       "         date  wm_yr_wk  snap_flag  \n",
       "0  2011-01-29     11101          0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test_not_in ('date') == dup of day_date_str\n",
    "test_in(['wm_yr_wk', 'snap_flag'], sample_melt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def join_w_prices(partition, raw_dir):\n",
    "    df_prices = pd.read_csv(f'{raw_dir}/sell_prices.csv')\n",
    "    partition = partition.merge(\n",
    "        df_prices,\n",
    "        on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "        how='left'\n",
    "    )\n",
    "    partition['sell_price'] = partition['sell_price'].astype('float32')\n",
    "    partition['sales_dollars'] = (partition['sales'] * partition['sell_price']).astype('float32')\n",
    "    partition = partition.fillna({'sales_dollars': 0}\n",
    "    # TODO: doesn't seem to be supported by dask, inplace=True\n",
    "    )\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-15 05:58:12] root: Copying 957.5 MiB (deep=True)\n",
      "DEBUG [2020-07-15 05:58:23] root: Copying 957.5 MiB (deep=True)\n"
     ]
    }
   ],
   "source": [
    "sample_melt = join_w_prices(sample_melt, raw_dir).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>day_date</th>\n",
       "      <th>day_date_str</th>\n",
       "      <th>month_id</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>snap_flag</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>sales_dollars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOUSEHOLD_1_365_CA_2_validation</td>\n",
       "      <td>HOUSEHOLD_1_365</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>CA_2</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>3.82</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id          item_id      dept_id     cat_id  \\\n",
       "0  HOUSEHOLD_1_365_CA_2_validation  HOUSEHOLD_1_365  HOUSEHOLD_1  HOUSEHOLD   \n",
       "\n",
       "  store_id state_id  day_id  sales   day_date day_date_str  month_id  \\\n",
       "0     CA_2       CA       1      5 2011-01-29   2011-01-29         1   \n",
       "\n",
       "         date  wm_yr_wk  snap_flag  sell_price  sales_dollars  \n",
       "0  2011-01-29     11101          0        3.82           19.1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in(['sell_price', 'sales_dollars'], sample_melt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da, dask.dataframe as dd\n",
    "\n",
    "def _reproduce_warn_generating_encoder():\n",
    "    c1 = da.concatenate([\n",
    "        da.from_array(\n",
    "            np.array(['c', 'a', 'b'])\n",
    "        ),\n",
    "        da.from_array(\n",
    "            np.array(['c', 'a', 'b'])\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    enc = dask_preprocessing.LabelEncoder()\n",
    "    c2 = enc.fit_transform(c1)\n",
    "    return enc\n",
    "\n",
    "enc = _reproduce_warn_generating_encoder()\n",
    "np.save('./tmp/encoder.npy', enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def save_encoder(enc, path):\n",
    "    classes_ = enc.classes_\n",
    "    if da.core.Array == type(enc.classes_):\n",
    "        classes_ = classes_.compute()\n",
    "    np.save(path, classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_encoder(enc, path='./tmp/encoder.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_parquet(sales_series, file_name, processed_dir, LOG):    \n",
    "    LOG.debug('Setting index')\n",
    "    sales_series = sales_series.set_index(sales_series['id'])\n",
    "    LOG.debug('Setting index - done')\n",
    "    encoders = {}\n",
    "    # TODO: dask supposedly does this on its own with sensible defaults\n",
    "    # sales_series['parquet_partition'] = np.random.randint(0, 100, sales_series.shape[0])\n",
    "\n",
    "    # this one is a dup of day_date_str which is harder to squeeze through the rest of the pipeline (yay petastorm)\n",
    "    if 'day_date' in sales_series.columns:\n",
    "        LOG.debug(f\"Dropping 'day_date' from {sales_series.columns}\")\n",
    "        sales_series = sales_series.drop(['day_date'], axis=1)\n",
    "\n",
    "    for col in sales_series.columns:\n",
    "        if col in encoders:\n",
    "            LOG.debug(f'Skipping: {col} - already encoded')\n",
    "            continue\n",
    "\n",
    "        # petastorm can't read these\n",
    "        if str(sales_series[col].dtype) == 'uint8':\n",
    "            sales_series[col] = sales_series[col].astype('int')\n",
    "\n",
    "        if str(sales_series[col].dtype) in ['category', 'object']:\n",
    "            LOG.debug(f'Encoding: {col}')            \n",
    "            enc = dask_preprocessing.LabelEncoder()\n",
    "            #enc = LabelEncoder()\n",
    "            sales_series[col] = enc.fit_transform(sales_series[col])\n",
    "            # TODO: update other transforms too!\n",
    "            encoders[col] = enc\n",
    "\n",
    "    for name, enc in encoders.items():\n",
    "        LOG.debug(f\"Saving encoder: {name}\")\n",
    "        save_encoder(enc, f'{processed_dir}/{name}.npy')\n",
    "\n",
    "    # TODO: uint -> int, category/object -> int, day_date -> drop\n",
    "    # TODO: this is being called both on dask and pandas data frames and args are rather not compatible :/\n",
    "    parquet_file = f'{processed_dir}/{file_name}'\n",
    "    LOG.debug(f\"Saving {type(sales_series)} to {parquet_file}\")\n",
    "    kwargs = {}\n",
    "    is_pandas_df = type(sales_series) == pd.DataFrame \n",
    "    index_kwarg_name = 'index' if is_pandas_df else 'write_index'\n",
    "    kwargs[index_kwarg_name] = False\n",
    "\n",
    "    sales_series.to_parquet(\n",
    "        parquet_file,\n",
    "        **kwargs\n",
    "#        partition_cols=['parquet_partition']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-15 06:04:50] root:  Setting index\n",
      "DEBUG [2020-07-15 06:04:50] root:  Setting index - done\n",
      "DEBUG [2020-07-15 06:04:50] root:  Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [2020-07-15 06:04:50] root:  Encoding: id\n",
      "DEBUG [2020-07-15 06:04:51] root:  Encoding: item_id\n",
      "DEBUG [2020-07-15 06:04:52] root:  Encoding: dept_id\n",
      "DEBUG [2020-07-15 06:04:54] root:  Encoding: cat_id\n",
      "DEBUG [2020-07-15 06:04:56] root:  Encoding: store_id\n",
      "DEBUG [2020-07-15 06:04:58] root:  Encoding: state_id\n",
      "DEBUG [2020-07-15 06:05:00] root:  Encoding: day_date_str\n",
      "DEBUG [2020-07-15 06:05:03] root:  Encoding: date\n",
      "DEBUG [2020-07-15 06:05:07] root:  Saving encoder: id\n",
      "DEBUG [2020-07-15 06:05:08] root:  Saving encoder: item_id\n",
      "DEBUG [2020-07-15 06:05:09] root:  Saving encoder: dept_id\n",
      "DEBUG [2020-07-15 06:05:11] root:  Saving encoder: cat_id\n",
      "DEBUG [2020-07-15 06:05:12] root:  Saving encoder: store_id\n",
      "DEBUG [2020-07-15 06:05:14] root:  Saving encoder: state_id\n",
      "DEBUG [2020-07-15 06:05:17] root:  Saving encoder: day_date_str\n",
      "DEBUG [2020-07-15 06:05:19] root:  Saving encoder: date\n",
      "DEBUG [2020-07-15 06:05:23] root:  Saving <class 'dask.dataframe.core.DataFrame'> to ./tmp/sample\n"
     ]
    }
   ],
   "source": [
    "to_parquet(sample_melt, 'sample', './tmp', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-15 06:05:35] root:  Setting index\n",
      "DEBUG [2020-07-15 06:05:36] root:  Setting index - done\n",
      "DEBUG [2020-07-15 06:05:36] root:  Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [2020-07-15 06:05:36] root:  Encoding: id\n",
      "DEBUG [2020-07-15 06:05:36] root:  Encoding: item_id\n",
      "DEBUG [2020-07-15 06:05:37] root:  Encoding: dept_id\n",
      "DEBUG [2020-07-15 06:05:39] root:  Encoding: cat_id\n",
      "DEBUG [2020-07-15 06:05:41] root:  Encoding: store_id\n",
      "DEBUG [2020-07-15 06:05:43] root:  Encoding: state_id\n",
      "DEBUG [2020-07-15 06:05:45] root:  Encoding: day_date_str\n",
      "DEBUG [2020-07-15 06:05:47] root:  Encoding: date\n",
      "DEBUG [2020-07-15 06:05:52] root:  Saving encoder: id\n",
      "DEBUG [2020-07-15 06:05:52] root:  Saving encoder: item_id\n",
      "DEBUG [2020-07-15 06:05:54] root:  Saving encoder: dept_id\n",
      "DEBUG [2020-07-15 06:05:55] root:  Saving encoder: cat_id\n",
      "DEBUG [2020-07-15 06:05:57] root:  Saving encoder: store_id\n",
      "DEBUG [2020-07-15 06:05:59] root:  Saving encoder: state_id\n",
      "DEBUG [2020-07-15 06:06:01] root:  Saving encoder: day_date_str\n",
      "DEBUG [2020-07-15 06:06:04] root:  Saving encoder: date\n",
      "DEBUG [2020-07-15 06:06:08] root:  Saving <class 'dask.dataframe.core.DataFrame'> to ./tmp/sample_pandas\n"
     ]
    }
   ],
   "source": [
    "pd_df = sample_melt.compute()\n",
    "test_eq(pd.DataFrame, type(pd_df))\n",
    "to_parquet(sample_melt, 'sample_pandas', './tmp', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id:object',\n",
       " 'item_id:object',\n",
       " 'dept_id:object',\n",
       " 'cat_id:object',\n",
       " 'store_id:object',\n",
       " 'state_id:object',\n",
       " 'day_id:int16',\n",
       " 'sales:int16',\n",
       " 'day_date:datetime64[ns]',\n",
       " 'day_date_str:object',\n",
       " 'month_id:uint8',\n",
       " 'date:object',\n",
       " 'wm_yr_wk:int16',\n",
       " 'snap_flag:uint8',\n",
       " 'sell_price:float32',\n",
       " 'sales_dollars:float32']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: check if these can be read back well with a sibling func\n",
    "[f'{col}:{sample_melt[col].dtype}' for col in sample_melt.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOUSEHOLD_2_195_CA_1_validation    1913\n",
       "HOUSEHOLD_1_502_WI_3_validation    1913\n",
       "HOUSEHOLD_1_477_CA_2_validation    1913\n",
       "HOUSEHOLD_1_399_CA_2_validation    1913\n",
       "HOUSEHOLD_1_389_TX_1_validation    1913\n",
       "HOUSEHOLD_1_365_CA_2_validation    1913\n",
       "HOUSEHOLD_1_115_WI_2_validation    1913\n",
       "HOUSEHOLD_1_029_CA_1_validation    1913\n",
       "HOBBIES_2_005_WI_2_validation      1913\n",
       "HOBBIES_1_300_WI_3_validation      1913\n",
       "FOODS_3_330_TX_2_validation        1913\n",
       "FOODS_3_105_WI_1_validation        1913\n",
       "FOODS_2_170_TX_1_validation        1913\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_melt['id'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def extract_id_columns(t):\n",
    "    extracted = t['id'].str.extract('([A-Z]+)_(\\\\d)_(\\\\d{3})_([A-Z]{2})_(\\d)')\n",
    "    t['cat_id'] = extracted[0]\n",
    "    t['dept_id'] = t['cat_id'] + '_' + extracted[1]\n",
    "    t['item_id'] = t['cat_id'] + '_' + extracted[2]\n",
    "    t['state_id'] = extracted[3]\n",
    "    t['store_id'] = t['state_id'] + '_' + extracted[4]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from datetime import timedelta\n",
    "def get_submission_template_melt(raw, d_1_date=pd.to_datetime('2016-06-20')):\n",
    "    df_sample_submission = pd.read_csv(f'{raw}/sample_submission.csv')\n",
    "\n",
    "    mapping = {f'F{day}':(d_1_date + timedelta(days=day-1)).date() for day in range(1,29)}\n",
    "    mapping['id'] = 'id'\n",
    "    df_sample_submission.columns = df_sample_submission.columns.map(mapping)\n",
    "    df_sample_submission_melt = df_sample_submission.melt(id_vars='id', var_name='day', value_name='sales')\n",
    "\n",
    "    last_prices = pd.read_csv(f'{raw}/sell_prices.csv')\n",
    "    max_week = last_prices['wm_yr_wk'].max()\n",
    "    last_prices = last_prices.query('wm_yr_wk == @max_week').copy()\n",
    "    last_prices['id'] = last_prices['item_id'] + '_' + last_prices['store_id']\n",
    "    last_prices_v = last_prices.copy()\n",
    "    last_prices_e = last_prices\n",
    "\n",
    "    last_prices_e['id'] = last_prices_e['id'] + '_evaluation'\n",
    "    last_prices_v['id'] = last_prices_v['id'] + '_validation'\n",
    "    last_prices = pd.concat([last_prices_e, last_prices_v], axis=0)[['id', 'sell_price']]    \n",
    "\n",
    "    df_sample_submission_melt = df_sample_submission_melt.merge(\n",
    "        last_prices, on='id', how='left', validate='many_to_one')\n",
    "\n",
    "    df_sample_submission_melt = extract_id_columns(df_sample_submission_melt)\n",
    "    df_sample_submission_melt.drop(['sales'], axis=1, inplace=True)\n",
    "    df_sample_submission_melt.rename({'day': 'date'}, axis=1, inplace=True)\n",
    "    return df_sample_submission_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_template = get_submission_template_melt(raw='raw')\n",
    "test_eq(pd.to_datetime('2016-06-20').date(), submission_template['date'].min())\n",
    "test_eq(2*30490, len(submission_template['id'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
