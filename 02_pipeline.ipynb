{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging already initialized\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# TODO: see if there are nicer ways of importing from other notebooks\n",
    "import sys\n",
    "sys.path.insert(1, './kaggle-m5-nbdev') \n",
    "from core import test_eq, test_err, configure_logging\n",
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from core import read_series_sample, melt_sales_series, extract_day_ids, join_w_calendar, join_w_prices\n",
    "from core import to_parquet, get_submission_template_melt\n",
    "import os\n",
    "\n",
    "def prepare_data_on_disk(log, n_sample_series, processed):\n",
    "    expected_path = f'{processed}/sales_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    log.info(f'Not found parquet file ({expected_path}) - preparing the data')\n",
    "\n",
    "    sales_series = read_series_sample(log, n_sample_series)\n",
    "    sales_series = melt_sales_series(sales_series)\n",
    "    sales_series = extract_day_ids(sales_series)\n",
    "    sales_series = join_w_calendar(sales_series)\n",
    "    sales_series = join_w_prices(sales_series).persist()\n",
    "    to_parquet(sales_series, 'sales_series_melt.parquet', processed, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " INFO [2020-07-13 20:19:09] root: Not found parquet file (./tmp/sales_series_melt.parquet) - preparing the data\n",
      "DEBUG [2020-07-13 20:19:14] root: Read 10 series\n",
      "DEBUG [2020-07-13 20:19:25] root: Setting index\n",
      "DEBUG [2020-07-13 20:19:25] root: Setting index - done\n",
      "DEBUG [2020-07-13 20:19:25] root: Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [2020-07-13 20:19:25] root: Encoding: id\n",
      "DEBUG [2020-07-13 20:19:26] root: Encoding: item_id\n",
      "DEBUG [2020-07-13 20:19:26] root: Encoding: dept_id\n",
      "DEBUG [2020-07-13 20:19:26] root: Encoding: cat_id\n",
      "DEBUG [2020-07-13 20:19:27] root: Encoding: store_id\n",
      "DEBUG [2020-07-13 20:19:27] root: Encoding: state_id\n",
      "DEBUG [2020-07-13 20:19:27] root: Encoding: day_date_str\n",
      "DEBUG [2020-07-13 20:19:28] root: Encoding: date\n",
      "DEBUG [2020-07-13 20:19:30] root: Saving encoder: id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-13 20:19:30] root: Saving encoder: item_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-13 20:19:30] root: Saving encoder: dept_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-13 20:19:30] root: Saving encoder: cat_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-13 20:19:31] root: Saving encoder: store_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-13 20:19:31] root: Saving encoder: state_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-13 20:19:31] root: Saving encoder: day_date_str\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-13 20:19:32] root: Saving encoder: date\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-13 20:19:33] root: Saving to ./tmp/sales_series_melt.parquet\n"
     ]
    }
   ],
   "source": [
    "prepare_data_on_disk(log, n_sample_series=10, processed='./tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to core (don't forget to add import here)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import dask.dataframe as dd\n",
    "def load_encoders(processed):\n",
    "    def _load(fn):\n",
    "        l = LabelEncoder()\n",
    "        l.classes_ = np.load(f'{processed}/{fn}', allow_pickle=True)\n",
    "        return l\n",
    "\n",
    "    encoders_paths = filter(lambda p: p.endswith('.npy'), os.listdir(processed))\n",
    "    encoders = {fn[:-len('.npy')]:_load(fn) for fn in encoders_paths}\n",
    "\n",
    "    return encoders\n",
    "\n",
    "def encode(log, me, processed):\n",
    "    encoders = load_encoders(processed)\n",
    "    continuous_cols = ['sell_price']\n",
    "\n",
    "    for col in me.columns:\n",
    "        dtype_str = str(me[col].dtype)\n",
    "        if col in continuous_cols:\n",
    "            log.debug(f\"Encoding {col} ({dtype_str}) as float32 just in case for pytorch\")\n",
    "            me[col] = me[col].astype('float32')\n",
    "            continue\n",
    "\n",
    "        log.debug(f\"Encoding {col} ({dtype_str}) as categorical \")\n",
    "\n",
    "        unlabelable = ~me[col].isin(encoders[col].classes_)\n",
    "        unlabelable_count = unlabelable.sum()\n",
    "        if unlabelable_count > 0:\n",
    "            default_label = encoders[col].classes_[0]\n",
    "            log.warning(f\"{unlabelable_count} entries for {col} can't be labeled. Defaulting to {default_label} e.g.\\n {me[unlabelable][col][:3].values}\")\n",
    "            me.loc[unlabelable, col] = default_label\n",
    "\n",
    "        me[col] = dd.from_array(encoders[col].transform(me[col]))\n",
    "\n",
    "    return me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'raw'\n",
    "processed = './tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data_on_disk(log, raw, processed):\n",
    "    expected_path = f'{processed}/test_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    template = get_submission_template_melt(raw)\n",
    "    test_data = encode(log, template, processed)\n",
    "    to_parquet(test_data, 'test_series_melt.parquet', processed, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-13 20:37:05] root: Encoding id (object) as categorical \n",
      "WARNING [2020-07-13 20:37:06] root: 1707076 entries for id can't be labeled. Defaulting to FOODS_1_132_CA_3_validation e.g.\n",
      " ['HOBBIES_1_001_CA_1_validation' 'HOBBIES_1_002_CA_1_validation'\n",
      " 'HOBBIES_1_003_CA_1_validation']\n",
      "DEBUG [2020-07-13 20:37:06] root: Encoding date (object) as categorical \n",
      "WARNING [2020-07-13 20:37:07] root: 1707440 entries for date can't be labeled. Defaulting to 2011-01-29 e.g.\n",
      " [datetime.date(2016, 6, 21) datetime.date(2016, 6, 21)\n",
      " datetime.date(2016, 6, 21)]\n",
      "DEBUG [2020-07-13 20:37:07] root: Encoding sell_price (float64) as float32 just in case for pytorch\n",
      "DEBUG [2020-07-13 20:37:07] root: Encoding cat_id (object) as categorical \n",
      "DEBUG [2020-07-13 20:37:08] root: Encoding dept_id (object) as categorical \n",
      "WARNING [2020-07-13 20:37:08] root: 83440 entries for dept_id can't be labeled. Defaulting to FOODS_1 e.g.\n",
      " ['HOBBIES_2' 'HOBBIES_2' 'HOBBIES_2']\n",
      "DEBUG [2020-07-13 20:37:09] root: Encoding item_id (object) as categorical \n",
      "WARNING [2020-07-13 20:37:09] root: 1707440 entries for item_id can't be labeled. Defaulting to FOODS_1_132 e.g.\n",
      " ['HOBBIES_001' 'HOBBIES_002' 'HOBBIES_003']\n",
      "DEBUG [2020-07-13 20:37:09] root: Encoding state_id (object) as categorical \n",
      "DEBUG [2020-07-13 20:37:10] root: Encoding store_id (object) as categorical \n",
      "WARNING [2020-07-13 20:37:10] root: 341488 entries for store_id can't be labeled. Defaulting to CA_1 e.g.\n",
      " ['WI_1' 'WI_1' 'WI_1']\n",
      "DEBUG [2020-07-13 20:37:11] root: Setting index\n",
      "DEBUG [2020-07-13 20:37:11] root: Setting index - done\n",
      "DEBUG [2020-07-13 20:37:11] root: Saving to ./tmp/test_series_melt.parquet\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__cinit__() got an unexpected keyword argument 'write_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-191dd78288fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprepare_test_data_on_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-0d7926718056>\u001b[0m in \u001b[0;36mprepare_test_data_on_disk\u001b[0;34m(log, raw, processed)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_submission_template_melt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_series_melt.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opower/kaggle-m5-acc-nbdev/kaggle-m5-nbdev/core.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(sales_series, file_name, processed_dir, LOG)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# writing index blows up dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# also below keyword is dask, pandas would be: index=False,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mwrite_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;31m#        partition_cols=['parquet_partition']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, fname, engine, compression, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m             \u001b[0mpartition_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2218\u001b[0m         )\n\u001b[1;32m   2219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mpartition_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, coerce_timestamps, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mcoerce_timestamps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_timestamps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             )\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                 \u001b[0muse_deprecated_int96_timestamps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_int96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 \u001b[0mcompression_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                 **kwargs) as writer:\n\u001b[0m\u001b[1;32m   1344\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_group_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_group_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, **options)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0muse_deprecated_int96_timestamps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_deprecated_int96_timestamps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mcompression_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             **options)\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_open\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __cinit__() got an unexpected keyword argument 'write_index'"
     ]
    }
   ],
   "source": [
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
