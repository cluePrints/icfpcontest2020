{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pipeline.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "from kaggle_m5_nbdev.core import test_eq, test_err, configure_logging\n",
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG', date_format='XXXX-XX-XX XX:XX:XX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from kaggle_m5_nbdev.core import read_series_sample, melt_sales_series, extract_day_ids, join_w_calendar, join_w_prices\n",
    "from kaggle_m5_nbdev.core import to_parquet, get_submission_template_melt\n",
    "from kaggle_m5_nbdev.petastorm import ParquetIterableDataset\n",
    "import os\n",
    "\n",
    "def prepare_data_on_disk(log, n_sample_series, processed_dir, raw_dir, force_data_prep):\n",
    "    expected_path = f'{processed_dir}/sales_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    log.info(f'Not found parquet file ({expected_path}) - preparing the data')\n",
    "\n",
    "    sales_series = read_series_sample(log, n_sample_series)\n",
    "    sales_series = melt_sales_series(sales_series)\n",
    "    sales_series = extract_day_ids(sales_series)\n",
    "    sales_series = join_w_calendar(sales_series, raw_dir)\n",
    "    sales_series = join_w_prices(sales_series, raw_dir).persist()\n",
    "    to_parquet(sales_series, 'sales_series_melt.parquet', processed_dir, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " INFO [XXXX-XX-XX XX:XX:XX] root:  Not found parquet file (./tmp/sales_series_melt.parquet) - preparing the data\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Read 10 series\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Setting index\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Setting index - done\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding: id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding: item_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding: dept_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding: cat_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding: store_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding: state_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding: day_date_str\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding: date\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving encoder: id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving encoder: item_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving encoder: dept_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving encoder: cat_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving encoder: store_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving encoder: state_id\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving encoder: day_date_str\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving encoder: date\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving <class 'dask.dataframe.core.DataFrame'> to ./tmp/sales_series_melt.parquet\n",
      " INFO [XXXX-XX-XX XX:XX:XX] root:  Found parquet file (./tmp/sales_series_melt.parquet)- skipping the prep\n"
     ]
    }
   ],
   "source": [
    "prepare_data_on_disk(log, n_sample_series=10, processed_dir='./tmp', raw_dir='raw', force_data_prep=True)\n",
    "prepare_data_on_disk(log, n_sample_series=10, processed_dir='./tmp', raw_dir='raw', force_data_prep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to core (don't forget to add import here)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "def load_encoders(processed):\n",
    "    def _load(fn):\n",
    "        l = LabelEncoder()\n",
    "        l.classes_ = np.load(f'{processed}/{fn}', allow_pickle=True)\n",
    "        return l\n",
    "\n",
    "    encoders_paths = filter(lambda p: p.endswith('.npy'), os.listdir(processed))\n",
    "    encoders = {fn[:-len('.npy')]:_load(fn) for fn in encoders_paths}\n",
    "\n",
    "    return encoders\n",
    "\n",
    "def encode(log, me, processed):\n",
    "    encoders = load_encoders(processed)\n",
    "    continuous_cols = ['sell_price']\n",
    "\n",
    "    for col in me.columns:\n",
    "        dtype_str = str(me[col].dtype)\n",
    "        if col in continuous_cols:\n",
    "            log.debug(f\"Encoding {col} ({dtype_str}) as float32 just in case for pytorch\")\n",
    "            me[col] = me[col].astype('float32')\n",
    "            continue\n",
    "\n",
    "        log.debug(f\"Encoding {col} ({dtype_str}) as categorical \")\n",
    "\n",
    "        unlabelable = ~me[col].isin(encoders[col].classes_)\n",
    "        unlabelable_count = unlabelable.sum()\n",
    "        if unlabelable_count > 0:\n",
    "            default_label = encoders[col].classes_[0]\n",
    "            log.warning(f\"{unlabelable_count} entries for {col} can't be labeled. Defaulting to {default_label} e.g.\\n {me[unlabelable][col][:3].values}\")\n",
    "            me.loc[unlabelable, col] = default_label\n",
    "\n",
    "        me[col] = encoders[col].transform(me[col])\n",
    "\n",
    "    return me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'raw'\n",
    "processed = './tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data_on_disk(log, raw, processed, force_data_prep):\n",
    "    expected_path = f'{processed}/test_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    template = get_submission_template_melt(raw)\n",
    "    test_data = encode(log, template, processed)\n",
    "    to_parquet(test_data, 'test_series_melt.parquet', processed, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding id (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root:   1707160 entries for id can't be labeled. Defaulting to FOODS_1_152_WI_3_validation e.g.\n",
      " ['HOBBIES_1_001_CA_1_validation' 'HOBBIES_1_002_CA_1_validation'\n",
      " 'HOBBIES_1_003_CA_1_validation']\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding date (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root:   1707440 entries for date can't be labeled. Defaulting to 2011-01-29 e.g.\n",
      " [datetime.date(2016, 6, 20) datetime.date(2016, 6, 20)\n",
      " datetime.date(2016, 6, 20)]\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding sell_price (float64) as float32 just in case for pytorch\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding cat_id (object) as categorical \n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding dept_id (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root:   455840 entries for dept_id can't be labeled. Defaulting to FOODS_1 e.g.\n",
      " ['HOBBIES_1' 'HOBBIES_1' 'HOBBIES_1']\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding item_id (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root:   1707440 entries for item_id can't be labeled. Defaulting to FOODS_1_152 e.g.\n",
      " ['HOBBIES_001' 'HOBBIES_002' 'HOBBIES_003']\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding state_id (object) as categorical \n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Encoding store_id (object) as categorical \n",
      "WARNING [XXXX-XX-XX XX:XX:XX] root:   512232 entries for store_id can't be labeled. Defaulting to CA_1 e.g.\n",
      " ['CA_3' 'CA_3' 'CA_3']\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Setting index\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Setting index - done\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   Saving <class 'pandas.core.frame.DataFrame'> to ./tmp/test_series_melt.parquet\n",
      " INFO [XXXX-XX-XX XX:XX:XX] root:  Found parquet file (./tmp/test_series_melt.parquet)- skipping the prep\n"
     ]
    }
   ],
   "source": [
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp', force_data_prep=True)\n",
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp', force_data_prep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader as TorchDataLoader, IterableDataset\n",
    "from collections import OrderedDict\n",
    "def setup_data_loaders(processed, log):\n",
    "    batch = 128\n",
    "\n",
    "    train_ds = ParquetIterableDataset(f'file:{processed}/sales_series_melt.parquet', log, '.*part.(?!1).*')\n",
    "    valid_ds = ParquetIterableDataset(f'file:{processed}/sales_series_melt.parquet', log, '.*part.1.*')\n",
    "    test_ds  = ParquetIterableDataset(f'file:{processed}/test_series_melt.parquet', log)\n",
    "\n",
    "    train_dl = TorchDataLoader(train_ds, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "    valid_dl = TorchDataLoader(valid_ds, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "    test_dl  = TorchDataLoader(test_ds,  batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    data = OrderedDict()\n",
    "    data[\"train\"] = train_dl\n",
    "    data[\"valid\"] = valid_dl\n",
    "    data[\"test\"]  = test_dl\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:    file:./tmp/sales_series_melt.parquet -> 1 files out of 4 remained after applying filter (.*part.(?!1).*)\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:    file:./tmp/sales_series_melt.parquet -> 1 files out of 4 remained after applying filter (.*part.1.*)\n"
     ]
    }
   ],
   "source": [
    "dls = setup_data_loaders(processed=processed, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_batch(model, data, log, log_stage):\n",
    "   for key, dl in data.items():\n",
    "        batch = next(iter(dl))\n",
    "        log.debug(f\"{key} model out @ {log_stage} {model.forward(batch['features']).transpose(1, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n",
      "/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.utils import set_global_seed\n",
    "\n",
    "class Net(nn.Sequential):\n",
    "    def __init__(self, num_features):\n",
    "        layers = []\n",
    "        layer_dims = [num_features, 200,200,20,20,1]\n",
    "        for in_features, out_features in zip(layer_dims[:-1], layer_dims[1:]):\n",
    "            l = nn.Linear(in_features, out_features)\n",
    "            # Note to self: loss @ init is quite important!\n",
    "            torch.nn.init.xavier_uniform_(l.weight) \n",
    "            torch.nn.init.zeros_(l.bias)\n",
    "\n",
    "            layers.append(l)\n",
    "            layers.append(nn.ReLU())\n",
    "        super(Net, self).__init__(*layers)\n",
    "\n",
    "class MyLoss(nn.MSELoss):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        return super().forward(inp, target)\n",
    "\n",
    "def do_train(data, log, log_dir):\n",
    "    model = Net(num_features = 2)\n",
    "    runner = SupervisedRunner()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    criterion = MyLoss()\n",
    "\n",
    "    log_batch(model, data, log, \"init\")\n",
    "\n",
    "    log.debug(\"Starting training\")\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        loaders=data,\n",
    "        logdir=f\"{log_dir}/run\",\n",
    "        load_best_on_end=True,\n",
    "        num_epochs=1)\n",
    "\n",
    "    log_batch(model, data, log, \"exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:       Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/compat.py:39: ResourceWarning:\n",
      "\n",
      "unclosed file <_io.BufferedReader name='./tmp/sales_series_melt.parquet/part.0.parquet'>\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/workers_pool/thread_pool.py:62: ResourceWarning:\n",
      "\n",
      "unclosed file <_io.BufferedReader name='./tmp/sales_series_melt.parquet/part.0.parquet'>\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning:\n",
      "\n",
      "Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/workers_pool/thread_pool.py:187: PendingDeprecationWarning:\n",
      "\n",
      "isAlive() is deprecated, use is_alive() instead\n",
      "\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   train model out @ init tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<TransposeBackward0>)\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:       Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/compat.py:39: ResourceWarning:\n",
      "\n",
      "unclosed file <_io.BufferedReader name='./tmp/sales_series_melt.parquet/part.1.parquet'>\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/workers_pool/thread_pool.py:62: ResourceWarning:\n",
      "\n",
      "unclosed file <_io.BufferedReader name='./tmp/sales_series_melt.parquet/part.1.parquet'>\n",
      "\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   valid model out @ init tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<TransposeBackward0>)\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:       Iterator created on file:./tmp/test_series_melt.parquet\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/compat.py:39: ResourceWarning:\n",
      "\n",
      "unclosed file <_io.BufferedReader name='./tmp/test_series_melt.parquet'>\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/workers_pool/thread_pool.py:62: ResourceWarning:\n",
      "\n",
      "unclosed file <_io.BufferedReader name='./tmp/test_series_melt.parquet'>\n",
      "\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:   test model out @ init tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<TransposeBackward0>)\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:  Starting training\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:            Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:            Done iterating: 45 batches / (5739 cells) \n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:            Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:            Done iterating: 105 batches / (13391 cells) \n",
      "DEBUG [XXXX-XX-XX XX:XX:XX] root:            Iterator created on file:./tmp/test_series_melt.parquet\n"
     ]
    }
   ],
   "source": [
    "do_train(dls, log, log_dir='./tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
