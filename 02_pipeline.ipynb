{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "from kaggle_m5_nbdev.core import test_eq, test_err, configure_logging\n",
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a445006ef50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_series_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmelt_sales_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_day_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_w_calendar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_w_prices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_submission_template_melt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'core'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "\n",
    "from core import read_series_sample, melt_sales_series, extract_day_ids, join_w_calendar, join_w_prices\n",
    "from core import to_parquet, get_submission_template_melt\n",
    "import os\n",
    "\n",
    "def prepare_data_on_disk(log, n_sample_series, processed_dir, raw_dir, force_data_prep):\n",
    "    expected_path = f'{processed_dir}/sales_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    log.info(f'Not found parquet file ({expected_path}) - preparing the data')\n",
    "\n",
    "    sales_series = read_series_sample(log, n_sample_series)\n",
    "    sales_series = melt_sales_series(sales_series)\n",
    "    sales_series = extract_day_ids(sales_series)\n",
    "    sales_series = join_w_calendar(sales_series, raw_dir)\n",
    "    sales_series = join_w_prices(sales_series, raw_dir).persist()\n",
    "    to_parquet(sales_series, 'sales_series_melt.parquet', processed_dir, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data_on_disk(log, n_sample_series=10, processed_dir='./tmp', raw_dir='raw', force_data_prep=True)\n",
    "prepare_data_on_disk(log, n_sample_series=10, processed_dir='./tmp', raw_dir='raw', force_data_prep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to core (don't forget to add import here)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import dask.dataframe as dd\n",
    "def load_encoders(processed):\n",
    "    def _load(fn):\n",
    "        l = LabelEncoder()\n",
    "        l.classes_ = np.load(f'{processed}/{fn}', allow_pickle=True)\n",
    "        return l\n",
    "\n",
    "    encoders_paths = filter(lambda p: p.endswith('.npy'), os.listdir(processed))\n",
    "    encoders = {fn[:-len('.npy')]:_load(fn) for fn in encoders_paths}\n",
    "\n",
    "    return encoders\n",
    "\n",
    "def encode(log, me, processed):\n",
    "    encoders = load_encoders(processed)\n",
    "    continuous_cols = ['sell_price']\n",
    "\n",
    "    for col in me.columns:\n",
    "        dtype_str = str(me[col].dtype)\n",
    "        if col in continuous_cols:\n",
    "            log.debug(f\"Encoding {col} ({dtype_str}) as float32 just in case for pytorch\")\n",
    "            me[col] = me[col].astype('float32')\n",
    "            continue\n",
    "\n",
    "        log.debug(f\"Encoding {col} ({dtype_str}) as categorical \")\n",
    "\n",
    "        unlabelable = ~me[col].isin(encoders[col].classes_)\n",
    "        unlabelable_count = unlabelable.sum()\n",
    "        if unlabelable_count > 0:\n",
    "            default_label = encoders[col].classes_[0]\n",
    "            log.warning(f\"{unlabelable_count} entries for {col} can't be labeled. Defaulting to {default_label} e.g.\\n {me[unlabelable][col][:3].values}\")\n",
    "            me.loc[unlabelable, col] = default_label\n",
    "\n",
    "        me[col] = encoders[col].transform(me[col])\n",
    "\n",
    "    return me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'raw'\n",
    "processed = './tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data_on_disk(log, raw, processed, force_data_prep):\n",
    "    expected_path = f'{processed}/test_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    template = get_submission_template_melt(raw)\n",
    "    test_data = encode(log, template, processed)\n",
    "    to_parquet(test_data, 'test_series_melt.parquet', processed, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp', force_data_prep=True)\n",
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp', force_data_prep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
