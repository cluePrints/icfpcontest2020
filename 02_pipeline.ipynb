{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "from kaggle_m5_nbdev.core import test_eq, test_err, configure_logging\n",
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from kaggle_m5_nbdev.core import read_series_sample, melt_sales_series, extract_day_ids, join_w_calendar, join_w_prices\n",
    "from kaggle_m5_nbdev.core import to_parquet, get_submission_template_melt\n",
    "import os\n",
    "\n",
    "def prepare_data_on_disk(log, n_sample_series, processed_dir, raw_dir, force_data_prep):\n",
    "    expected_path = f'{processed_dir}/sales_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    log.info(f'Not found parquet file ({expected_path}) - preparing the data')\n",
    "\n",
    "    sales_series = read_series_sample(log, n_sample_series)\n",
    "    sales_series = melt_sales_series(sales_series)\n",
    "    sales_series = extract_day_ids(sales_series)\n",
    "    sales_series = join_w_calendar(sales_series, raw_dir)\n",
    "    sales_series = join_w_prices(sales_series, raw_dir).persist()\n",
    "    to_parquet(sales_series, 'sales_series_melt.parquet', processed_dir, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " INFO [2020-07-29 06:37:20] root: Not found parquet file (./tmp/sales_series_melt.parquet) - preparing the data\n",
      "DEBUG [2020-07-29 06:37:27] root: Read 10 series\n",
      "DEBUG [2020-07-29 06:37:39] root: Setting index\n",
      "DEBUG [2020-07-29 06:37:39] root: Setting index - done\n",
      "DEBUG [2020-07-29 06:37:39] root: Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [2020-07-29 06:37:39] root: Encoding: id\n",
      "DEBUG [2020-07-29 06:37:40] root: Encoding: item_id\n",
      "DEBUG [2020-07-29 06:37:40] root: Encoding: dept_id\n",
      "DEBUG [2020-07-29 06:37:40] root: Encoding: cat_id\n",
      "DEBUG [2020-07-29 06:37:40] root: Encoding: store_id\n",
      "DEBUG [2020-07-29 06:37:41] root: Encoding: state_id\n",
      "DEBUG [2020-07-29 06:37:41] root: Encoding: day_date_str\n",
      "DEBUG [2020-07-29 06:37:41] root: Encoding: date\n",
      "DEBUG [2020-07-29 06:37:43] root: Saving encoder: id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-29 06:37:44] root: Saving encoder: item_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-29 06:37:44] root: Saving encoder: dept_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-29 06:37:44] root: Saving encoder: cat_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-29 06:37:45] root: Saving encoder: store_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-29 06:37:45] root: Saving encoder: state_id\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-29 06:37:45] root: Saving encoder: day_date_str\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-29 06:37:46] root: Saving encoder: date\n",
      "/usr/local/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.save` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "DEBUG [2020-07-29 06:37:48] root: Saving <class 'dask.dataframe.core.DataFrame'> to ./tmp/sales_series_melt.parquet\n",
      " INFO [2020-07-29 06:37:57] root: Found parquet file (./tmp/sales_series_melt.parquet)- skipping the prep\n"
     ]
    }
   ],
   "source": [
    "prepare_data_on_disk(log, n_sample_series=10, processed_dir='./tmp', raw_dir='raw', force_data_prep=True)\n",
    "prepare_data_on_disk(log, n_sample_series=10, processed_dir='./tmp', raw_dir='raw', force_data_prep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to core (don't forget to add import here)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "def load_encoders(processed):\n",
    "    def _load(fn):\n",
    "        l = LabelEncoder()\n",
    "        l.classes_ = np.load(f'{processed}/{fn}', allow_pickle=True)\n",
    "        return l\n",
    "\n",
    "    encoders_paths = filter(lambda p: p.endswith('.npy'), os.listdir(processed))\n",
    "    encoders = {fn[:-len('.npy')]:_load(fn) for fn in encoders_paths}\n",
    "\n",
    "    return encoders\n",
    "\n",
    "def encode(log, me, processed):\n",
    "    encoders = load_encoders(processed)\n",
    "    continuous_cols = ['sell_price']\n",
    "\n",
    "    for col in me.columns:\n",
    "        dtype_str = str(me[col].dtype)\n",
    "        if col in continuous_cols:\n",
    "            log.debug(f\"Encoding {col} ({dtype_str}) as float32 just in case for pytorch\")\n",
    "            me[col] = me[col].astype('float32')\n",
    "            continue\n",
    "\n",
    "        log.debug(f\"Encoding {col} ({dtype_str}) as categorical \")\n",
    "\n",
    "        unlabelable = ~me[col].isin(encoders[col].classes_)\n",
    "        unlabelable_count = unlabelable.sum()\n",
    "        if unlabelable_count > 0:\n",
    "            default_label = encoders[col].classes_[0]\n",
    "            log.warning(f\"{unlabelable_count} entries for {col} can't be labeled. Defaulting to {default_label} e.g.\\n {me[unlabelable][col][:3].values}\")\n",
    "            me.loc[unlabelable, col] = default_label\n",
    "\n",
    "        me[col] = encoders[col].transform(me[col])\n",
    "\n",
    "    return me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'raw'\n",
    "processed = './tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data_on_disk(log, raw, processed, force_data_prep):\n",
    "    expected_path = f'{processed}/test_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    template = get_submission_template_melt(raw)\n",
    "    test_data = encode(log, template, processed)\n",
    "    to_parquet(test_data, 'test_series_melt.parquet', processed, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-29 06:38:09] root:   Encoding id (object) as categorical \n",
      "WARNING [2020-07-29 06:38:09] root:   1707160 entries for id can't be labeled. Defaulting to FOODS_1_083_WI_3_validation e.g.\n",
      " ['HOBBIES_1_001_CA_1_validation' 'HOBBIES_1_002_CA_1_validation'\n",
      " 'HOBBIES_1_003_CA_1_validation']\n",
      "DEBUG [2020-07-29 06:38:10] root:   Encoding date (object) as categorical \n",
      "WARNING [2020-07-29 06:38:10] root:   1707440 entries for date can't be labeled. Defaulting to 2011-01-29 e.g.\n",
      " [datetime.date(2016, 6, 20) datetime.date(2016, 6, 20)\n",
      " datetime.date(2016, 6, 20)]\n",
      "DEBUG [2020-07-29 06:38:11] root:   Encoding sell_price (float64) as float32 just in case for pytorch\n",
      "DEBUG [2020-07-29 06:38:11] root:   Encoding cat_id (object) as categorical \n",
      "DEBUG [2020-07-29 06:38:11] root:   Encoding dept_id (object) as categorical \n",
      "WARNING [2020-07-29 06:38:11] root:   306320 entries for dept_id can't be labeled. Defaulting to FOODS_1 e.g.\n",
      " ['HOBBIES_2' 'HOBBIES_2' 'HOBBIES_2']\n",
      "DEBUG [2020-07-29 06:38:12] root:   Encoding item_id (object) as categorical \n",
      "WARNING [2020-07-29 06:38:12] root:   1707440 entries for item_id can't be labeled. Defaulting to FOODS_1_083 e.g.\n",
      " ['HOBBIES_001' 'HOBBIES_002' 'HOBBIES_003']\n",
      "DEBUG [2020-07-29 06:38:13] root:   Encoding state_id (object) as categorical \n",
      "DEBUG [2020-07-29 06:38:13] root:   Encoding store_id (object) as categorical \n",
      "WARNING [2020-07-29 06:38:13] root:   341488 entries for store_id can't be labeled. Defaulting to CA_2 e.g.\n",
      " ['CA_1' 'CA_1' 'CA_1']\n",
      "DEBUG [2020-07-29 06:38:14] root:   Setting index\n",
      "DEBUG [2020-07-29 06:38:14] root:   Setting index - done\n",
      "DEBUG [2020-07-29 06:38:14] root:   Saving <class 'pandas.core.frame.DataFrame'> to ./tmp/test_series_melt.parquet\n",
      " INFO [2020-07-29 06:38:15] root:  Found parquet file (./tmp/test_series_melt.parquet)- skipping the prep\n"
     ]
    }
   ],
   "source": [
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp', force_data_prep=True)\n",
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp', force_data_prep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
