{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "from kaggle_m5_nbdev.core import test_eq, test_err, configure_logging\n",
    "log = configure_logging('./tmp', 'test_log', con_log_lvl='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from kaggle_m5_nbdev.core import read_series_sample, melt_sales_series, extract_day_ids, join_w_calendar, join_w_prices\n",
    "from kaggle_m5_nbdev.core import to_parquet, get_submission_template_melt\n",
    "from kaggle_m5_nbdev.petastorm import ParquetIterableDataset\n",
    "import os\n",
    "\n",
    "def prepare_data_on_disk(log, n_sample_series, processed_dir, raw_dir, force_data_prep):\n",
    "    expected_path = f'{processed_dir}/sales_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    log.info(f'Not found parquet file ({expected_path}) - preparing the data')\n",
    "\n",
    "    sales_series = read_series_sample(log, n_sample_series)\n",
    "    sales_series = melt_sales_series(sales_series)\n",
    "    sales_series = extract_day_ids(sales_series)\n",
    "    sales_series = join_w_calendar(sales_series, raw_dir)\n",
    "    sales_series = join_w_prices(sales_series, raw_dir).persist()\n",
    "    to_parquet(sales_series, 'sales_series_melt.parquet', processed_dir, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " INFO [2020-07-30 05:57:00] root:  Not found parquet file (./tmp/sales_series_melt.parquet) - preparing the data\n",
      "DEBUG [2020-07-30 05:57:06] root:   Read 10 series\n",
      "DEBUG [2020-07-30 05:57:18] root:   Setting index\n",
      "DEBUG [2020-07-30 05:57:18] root:   Setting index - done\n",
      "DEBUG [2020-07-30 05:57:18] root:   Dropping 'day_date' from Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day_id',\n",
      "       'sales', 'day_date', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
      "       'snap_flag', 'sell_price', 'sales_dollars'],\n",
      "      dtype='object')\n",
      "DEBUG [2020-07-30 05:57:18] root:   Encoding: id\n",
      "DEBUG [2020-07-30 05:57:18] root:   Encoding: item_id\n",
      "DEBUG [2020-07-30 05:57:18] root:   Encoding: dept_id\n",
      "DEBUG [2020-07-30 05:57:18] root:   Encoding: cat_id\n",
      "DEBUG [2020-07-30 05:57:19] root:   Encoding: store_id\n",
      "DEBUG [2020-07-30 05:57:19] root:   Encoding: state_id\n",
      "DEBUG [2020-07-30 05:57:20] root:   Encoding: day_date_str\n",
      "DEBUG [2020-07-30 05:57:20] root:   Encoding: date\n",
      "DEBUG [2020-07-30 05:57:22] root:   Saving encoder: id\n",
      "DEBUG [2020-07-30 05:57:22] root:   Saving encoder: item_id\n",
      "DEBUG [2020-07-30 05:57:22] root:   Saving encoder: dept_id\n",
      "DEBUG [2020-07-30 05:57:22] root:   Saving encoder: cat_id\n",
      "DEBUG [2020-07-30 05:57:23] root:   Saving encoder: store_id\n",
      "DEBUG [2020-07-30 05:57:23] root:   Saving encoder: state_id\n",
      "DEBUG [2020-07-30 05:57:23] root:   Saving encoder: day_date_str\n",
      "DEBUG [2020-07-30 05:57:24] root:   Saving encoder: date\n",
      "DEBUG [2020-07-30 05:57:26] root:   Saving <class 'dask.dataframe.core.DataFrame'> to ./tmp/sales_series_melt.parquet\n",
      " INFO [2020-07-30 05:57:30] root:  Found parquet file (./tmp/sales_series_melt.parquet)- skipping the prep\n"
     ]
    }
   ],
   "source": [
    "prepare_data_on_disk(log, n_sample_series=10, processed_dir='./tmp', raw_dir='raw', force_data_prep=True)\n",
    "prepare_data_on_disk(log, n_sample_series=10, processed_dir='./tmp', raw_dir='raw', force_data_prep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to core (don't forget to add import here)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "def load_encoders(processed):\n",
    "    def _load(fn):\n",
    "        l = LabelEncoder()\n",
    "        l.classes_ = np.load(f'{processed}/{fn}', allow_pickle=True)\n",
    "        return l\n",
    "\n",
    "    encoders_paths = filter(lambda p: p.endswith('.npy'), os.listdir(processed))\n",
    "    encoders = {fn[:-len('.npy')]:_load(fn) for fn in encoders_paths}\n",
    "\n",
    "    return encoders\n",
    "\n",
    "def encode(log, me, processed):\n",
    "    encoders = load_encoders(processed)\n",
    "    continuous_cols = ['sell_price']\n",
    "\n",
    "    for col in me.columns:\n",
    "        dtype_str = str(me[col].dtype)\n",
    "        if col in continuous_cols:\n",
    "            log.debug(f\"Encoding {col} ({dtype_str}) as float32 just in case for pytorch\")\n",
    "            me[col] = me[col].astype('float32')\n",
    "            continue\n",
    "\n",
    "        log.debug(f\"Encoding {col} ({dtype_str}) as categorical \")\n",
    "\n",
    "        unlabelable = ~me[col].isin(encoders[col].classes_)\n",
    "        unlabelable_count = unlabelable.sum()\n",
    "        if unlabelable_count > 0:\n",
    "            default_label = encoders[col].classes_[0]\n",
    "            log.warning(f\"{unlabelable_count} entries for {col} can't be labeled. Defaulting to {default_label} e.g.\\n {me[unlabelable][col][:3].values}\")\n",
    "            me.loc[unlabelable, col] = default_label\n",
    "\n",
    "        me[col] = encoders[col].transform(me[col])\n",
    "\n",
    "    return me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'raw'\n",
    "processed = './tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data_on_disk(log, raw, processed, force_data_prep):\n",
    "    expected_path = f'{processed}/test_series_melt.parquet'\n",
    "    if os.path.exists(expected_path) and not force_data_prep:\n",
    "        log.info(f'Found parquet file ({expected_path})- skipping the prep')\n",
    "        return\n",
    "\n",
    "    template = get_submission_template_melt(raw)\n",
    "    test_data = encode(log, template, processed)\n",
    "    to_parquet(test_data, 'test_series_melt.parquet', processed, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-30 05:57:40] root:   Encoding id (object) as categorical \n",
      "WARNING [2020-07-30 05:57:41] root:   1707160 entries for id can't be labeled. Defaulting to FOODS_3_551_CA_3_validation e.g.\n",
      " ['HOBBIES_1_001_CA_1_validation' 'HOBBIES_1_002_CA_1_validation'\n",
      " 'HOBBIES_1_003_CA_1_validation']\n",
      "DEBUG [2020-07-30 05:57:41] root:   Encoding date (object) as categorical \n",
      "WARNING [2020-07-30 05:57:42] root:   1707440 entries for date can't be labeled. Defaulting to 2011-01-29 e.g.\n",
      " [datetime.date(2016, 6, 20) datetime.date(2016, 6, 20)\n",
      " datetime.date(2016, 6, 20)]\n",
      "DEBUG [2020-07-30 05:57:42] root:   Encoding sell_price (float64) as float32 just in case for pytorch\n",
      "DEBUG [2020-07-30 05:57:42] root:   Encoding cat_id (object) as categorical \n",
      "DEBUG [2020-07-30 05:57:43] root:   Encoding dept_id (object) as categorical \n",
      "WARNING [2020-07-30 05:57:43] root:   343840 entries for dept_id can't be labeled. Defaulting to FOODS_3 e.g.\n",
      " ['FOODS_1' 'FOODS_1' 'FOODS_1']\n",
      "DEBUG [2020-07-30 05:57:43] root:   Encoding item_id (object) as categorical \n",
      "WARNING [2020-07-30 05:57:44] root:   1707440 entries for item_id can't be labeled. Defaulting to FOODS_3_551 e.g.\n",
      " ['HOBBIES_001' 'HOBBIES_002' 'HOBBIES_003']\n",
      "DEBUG [2020-07-30 05:57:44] root:   Encoding state_id (object) as categorical \n",
      "DEBUG [2020-07-30 05:57:45] root:   Encoding store_id (object) as categorical \n",
      "WARNING [2020-07-30 05:57:45] root:   341488 entries for store_id can't be labeled. Defaulting to CA_1 e.g.\n",
      " ['TX_1' 'TX_1' 'TX_1']\n",
      "DEBUG [2020-07-30 05:57:45] root:   Setting index\n",
      "DEBUG [2020-07-30 05:57:45] root:   Setting index - done\n",
      "DEBUG [2020-07-30 05:57:45] root:   Saving <class 'pandas.core.frame.DataFrame'> to ./tmp/test_series_melt.parquet\n",
      " INFO [2020-07-30 05:57:45] root:  Found parquet file (./tmp/test_series_melt.parquet)- skipping the prep\n"
     ]
    }
   ],
   "source": [
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp', force_data_prep=True)\n",
    "prepare_test_data_on_disk(log, raw='raw', processed='./tmp', force_data_prep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader as TorchDataLoader, IterableDataset\n",
    "from collections import OrderedDict\n",
    "def setup_data_loaders(processed, log):\n",
    "    batch = 128\n",
    "\n",
    "    train_ds = ParquetIterableDataset(f'file:{processed}/sales_series_melt.parquet', log, '.*part.(?!1).*')\n",
    "    valid_ds = ParquetIterableDataset(f'file:{processed}/sales_series_melt.parquet', log, '.*part.1.*')\n",
    "    test_ds  = ParquetIterableDataset(f'file:{processed}/test_series_melt.parquet', log)\n",
    "\n",
    "    train_dl = TorchDataLoader(train_ds, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "    valid_dl = TorchDataLoader(valid_ds, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "    test_dl  = TorchDataLoader(test_ds,  batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    data = OrderedDict()\n",
    "    data[\"train\"] = train_dl\n",
    "    data[\"valid\"] = valid_dl\n",
    "    data[\"test\"]  = test_dl\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-30 06:01:19] root: file:./tmp/sales_series_melt.parquet -> 1 files out of 4 remained after applying filter (.*part.(?!1).*)\n",
      "DEBUG [2020-07-30 06:01:19] root: file:./tmp/sales_series_melt.parquet -> 1 files out of 4 remained after applying filter (.*part.1.*)\n"
     ]
    }
   ],
   "source": [
    "dls = setup_data_loaders(processed=processed, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_batch(model, data, log, log_stage):\n",
    "   for key, dl in data.items():\n",
    "        batch = next(iter(dl))\n",
    "        log.debug(f\"{key} model out @ {log_stage} {model.forward(batch['features']).transpose(1, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.utils import set_global_seed\n",
    "\n",
    "class Net(nn.Sequential):\n",
    "    def __init__(self, num_features):\n",
    "        layers = []\n",
    "        layer_dims = [num_features, 200,200,20,20,1]\n",
    "        for in_features, out_features in zip(layer_dims[:-1], layer_dims[1:]):\n",
    "            l = nn.Linear(in_features, out_features)\n",
    "            # Note to self: loss @ init is quite important!\n",
    "            torch.nn.init.xavier_uniform_(l.weight) \n",
    "            torch.nn.init.zeros_(l.bias)\n",
    "\n",
    "            layers.append(l)\n",
    "            layers.append(nn.ReLU())\n",
    "        super(Net, self).__init__(*layers)\n",
    "\n",
    "class MyLoss(nn.MSELoss):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        return super().forward(inp, target)\n",
    "\n",
    "def do_train(data, log, log_dir):\n",
    "    model = Net(num_features = 2)\n",
    "    runner = SupervisedRunner()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    criterion = MyLoss()\n",
    "\n",
    "    log_batch(model, data, log, \"init\")\n",
    "\n",
    "    log.debug(\"Starting training\")\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        loaders=data,\n",
    "        logdir=f\"{log_dir}/run\",\n",
    "        load_best_on_end=True,\n",
    "        num_epochs=1)\n",
    "\n",
    "    log_batch(model, data, log, \"exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-30 06:04:50] root:       Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "DEBUG [2020-07-30 06:04:52] root:   train model out @ init tensor([[0.1658, 0.1949, 0.1874, 0.1874, 0.0416, 0.1658, 0.1949, 0.0825, 0.0825,\n",
      "         0.1824, 0.0825, 0.1658, 0.1824, 0.1949, 0.1974, 0.1949, 0.1824, 0.0825,\n",
      "         0.1824, 0.1658, 0.1949, 0.1949, 0.1824, 0.0825, 0.0825, 0.1658, 0.1824,\n",
      "         0.1658, 0.0825, 0.1949, 0.1658, 0.1949, 0.1949, 0.1949, 0.1824, 0.0825,\n",
      "         0.1658, 0.0825, 0.1874, 0.1949, 0.1658, 0.1824, 0.1658, 0.0825, 0.0825,\n",
      "         0.1658, 0.1658, 0.0825, 0.1658, 0.1658, 0.0825, 0.0825, 0.1949, 0.0825,\n",
      "         0.0825, 0.1658, 0.0825, 0.0825, 0.0825, 0.1824, 0.1658, 0.1949, 0.1658,\n",
      "         0.0825, 0.1949, 0.1666, 0.1658, 0.1658, 0.1949, 0.1949, 0.0825, 0.1949,\n",
      "         0.0825, 0.0825, 0.0825, 0.1949, 0.1824, 0.1658, 0.0825, 0.1874, 0.0825,\n",
      "         0.1658, 0.1824, 0.1949, 0.1658, 0.1658, 0.0825, 0.1949, 0.0825, 0.0825,\n",
      "         0.1824, 0.1658, 0.1949, 0.0825, 0.0825, 0.1949, 0.1874, 0.1658, 0.0825,\n",
      "         0.1658, 0.0825, 0.1949, 0.0825, 0.1949, 0.0825, 0.0825, 0.1658, 0.1658,\n",
      "         0.1658, 0.0825, 0.1974, 0.1949, 0.0825, 0.0825, 0.0825, 0.0825, 0.0825,\n",
      "         0.1949, 0.0825, 0.1658, 0.1658, 0.1949, 0.1974, 0.0825, 0.1874, 0.1824,\n",
      "         0.1658, 0.1824]], grad_fn=<TransposeBackward0>)\n",
      "DEBUG [2020-07-30 06:04:52] root:       Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "DEBUG [2020-07-30 06:04:53] root:   valid model out @ init tensor([[0.4156, 0.1653, 0.1199, 0.1658, 0.1616, 0.3074, 0.0120, 0.1658, 0.2782,\n",
      "         0.1949, 0.2782, 0.2132, 0.2782, 0.1653, 0.2066, 0.1658, 0.2066, 0.4156,\n",
      "         0.1154, 0.1533, 0.2066, 0.2132, 0.1154, 0.4156, 0.1653, 0.1075, 0.1658,\n",
      "         0.0120, 0.4156, 0.2782, 0.1949, 0.4156, 0.1949, 0.1075, 0.2782, 0.1616,\n",
      "         0.1075, 0.2782, 0.1949, 0.0120, 0.2066, 0.2782, 0.2066, 0.1616, 0.1154,\n",
      "         0.1783, 0.0120, 0.1533, 0.4156, 0.1653, 0.2132, 0.0120, 0.0120, 0.1154,\n",
      "         0.1783, 0.1658, 0.2782, 0.1616, 0.2782, 0.1533, 0.1653, 0.1949, 0.2132,\n",
      "         0.2782, 0.1424, 0.1199, 0.1783, 0.2066, 0.1658, 0.4156, 0.1616, 0.1616,\n",
      "         0.3074, 0.1154, 0.2066, 0.2066, 0.1533, 0.3074, 0.1075, 0.2132, 0.1154,\n",
      "         0.1533, 0.1637, 0.1637, 0.1533, 0.0120, 0.1199, 0.1637, 0.3074, 0.4156,\n",
      "         0.0120, 0.4156, 0.1199, 0.1949, 0.1658, 0.2066, 0.2066, 0.3074, 0.2782,\n",
      "         0.4156, 0.4156, 0.2066, 0.1653, 0.4156, 0.2782, 0.3736, 0.2132, 0.4156,\n",
      "         0.3074, 0.0120, 0.2066, 0.1154, 0.1637, 0.1949, 0.4156, 0.1637, 0.2066,\n",
      "         0.1424, 0.1199, 0.4156, 0.3074, 0.1199, 0.1154, 0.2132, 0.3074, 0.4156,\n",
      "         0.1616, 0.1533]], grad_fn=<TransposeBackward0>)\n",
      "DEBUG [2020-07-30 06:04:53] root:       Iterator created on file:./tmp/test_series_melt.parquet\n",
      "DEBUG [2020-07-30 06:04:57] root:   test model out @ init tensor([[0.0391, 0.1474, 0.1241, 0.1033, 0.1237, 0.2070, 0.1241, 0.2016, 0.5239,\n",
      "         0.1241, 0.2195, 0.1508, 0.5822, 0.1533, 0.2174, 0.2491, 0.0416, 0.1424,\n",
      "         0.3111, 0.3740, 0.2782, 0.1237, 0.2186, 0.1387, 0.0779, 0.0983, 0.0575,\n",
      "         0.2699, 0.0991, 0.0933, 0.2903, 0.1508, 0.0741, 0.2074, 0.1758, 0.2266,\n",
      "         0.0825, 0.3565, 0.1349, 0.3319, 0.1241, 0.0404, 0.1449, 0.1237, 0.1449,\n",
      "         0.1241, 0.1199, 0.1574, 0.1241, 0.2195, 0.0779, 0.0308, 0.1041, 0.0808,\n",
      "         0.2182, 0.2907, 0.1433, 0.1241, 0.6439, 0.1616, 0.3698, 0.1949, 0.2432,\n",
      "         0.2486, 0.2049, 0.1008, 0.4152, 0.3973, 0.1033, 0.3319, 0.0820, 0.0933,\n",
      "         0.1241, 0.2278, 0.3923, 0.1033, 0.2278, 0.1241, 0.0825, 0.2674, 0.2016,\n",
      "         0.3319, 0.1141, 0.2907, 0.5348, 0.1991, 0.2182, 0.2432, 0.1616, 0.0820,\n",
      "         0.2853, 0.1849, 0.0404, 0.3736, 0.1199, 0.1116, 0.0333, 0.1658, 0.1224,\n",
      "         0.1653, 0.0416, 0.1224, 0.3732, 0.0816, 0.1041, 0.1237, 0.2278, 0.4056,\n",
      "         0.1653, 0.0700, 0.1491, 0.0096, 0.0404, 0.0625, 0.4140, 0.4140, 0.1658,\n",
      "         0.1041, 0.4531, 0.1237, 0.0816, 0.1449, 0.3115, 0.1658, 0.0408, 0.0737,\n",
      "         0.3282, 0.0200]], grad_fn=<TransposeBackward0>)\n",
      "DEBUG [2020-07-30 06:04:57] root:  Starting training\n",
      "DEBUG [2020-07-30 06:04:57] root:            Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "DEBUG [2020-07-30 06:04:58] root:            Done iterating: 45 batches / (5739 cells) \n",
      "DEBUG [2020-07-30 06:04:59] root:            Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "DEBUG [2020-07-30 06:05:01] root:            Done iterating: 105 batches / (13391 cells) \n",
      "DEBUG [2020-07-30 06:05:01] root:            Iterator created on file:./tmp/test_series_melt.parquet\n",
      "DEBUG [2020-07-30 06:06:29] root:            Done iterating: 13340 batches / (1707440 cells) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-30 06:06:29,955] \n",
      "1/1 * Epoch 1 (_base): lr=0.0100 | momentum=0.9000\n",
      "1/1 * Epoch 1 (train): loss=56.5099\n",
      "1/1 * Epoch 1 (valid): loss=87.0118\n",
      "1/1 * Epoch 1 (test): loss=143.3571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-30 06:06:30] root:       Iterator created on file:./tmp/sales_series_melt.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top best models:\n",
      "tmp/run/checkpoints/train.1.pth\t87.0118\n",
      "=> Loading checkpoint tmp/run/checkpoints/best_full.pth\n",
      "loaded state checkpoint tmp/run/checkpoints/best_full.pth (global epoch 1, epoch 1, stage train)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG [2020-07-30 06:06:31] root:   train model out @ exit tensor([[8.7125, 3.5739, 9.0780, 3.5739, 7.6970, 3.5739, 9.0780, 3.5739, 3.5739,\n",
      "         3.5739, 9.0780, 7.6564, 3.5739, 3.5739, 7.6970, 9.0780, 9.0780, 3.5739,\n",
      "         3.5739, 7.6564, 9.0780, 9.0780, 7.6564, 7.6564, 3.5739, 3.5739, 7.6564,\n",
      "         7.6564, 9.0780, 3.5739, 3.5739, 9.0780, 8.4688, 9.0780, 7.6564, 9.0780,\n",
      "         9.0780, 3.5739, 9.0780, 9.0780, 9.0780, 3.5739, 8.4688, 3.5739, 8.4688,\n",
      "         8.7125, 9.0780, 3.5739, 3.5739, 9.0780, 3.5739, 3.5739, 8.4688, 7.6564,\n",
      "         9.0780, 9.0780, 9.0780, 9.0780, 7.6564, 8.4688, 7.6564, 8.4688, 3.5739,\n",
      "         3.5739, 9.0780, 3.5739, 7.6564, 9.0780, 9.0780, 3.5739, 9.0780, 3.5739,\n",
      "         7.6564, 3.5739, 7.6564, 3.5739, 9.0780, 3.5739, 7.6564, 3.5739, 3.5739,\n",
      "         9.0780, 7.6564, 3.5739, 3.5739, 9.0780, 3.5739, 3.5739, 3.5739, 3.5739,\n",
      "         9.0780, 3.5739, 8.4688, 8.4688, 3.5739, 7.6564, 7.6564, 7.6564, 7.6564,\n",
      "         8.4688, 3.5739, 9.0780, 7.6564, 7.6564, 9.0780, 3.5739, 7.6970, 7.6564,\n",
      "         7.6564, 9.0780, 7.6564, 8.4688, 8.4688, 3.5739, 7.6564, 3.5739, 8.4688,\n",
      "         7.6564, 7.6564, 3.5739, 7.6564, 9.0780, 3.5739, 7.6970, 3.5739, 7.6564,\n",
      "         8.4688, 7.6564]], grad_fn=<TransposeBackward0>)\n",
      "DEBUG [2020-07-30 06:06:31] root:       Iterator created on file:./tmp/sales_series_melt.parquet\n",
      "DEBUG [2020-07-30 06:06:33] root:   valid model out @ exit tensor([[13.0915,  5.4031, 13.0915,  7.6361, 19.6995,  7.5544, 13.0915,  6.5084,\n",
      "         19.6995,  8.2657,  5.1785,  4.7916,  9.9636,  7.6361, 19.6995,  6.5084,\n",
      "          8.2657,  7.4518, 19.6995,  7.5544,  0.0000, 17.6770,  9.9636,  0.0000,\n",
      "          7.6361,  9.0780,  9.6427, 19.6995, 19.6995,  7.6361,  5.4031, 19.6995,\n",
      "          6.5084,  5.1785,  0.0000,  5.4031,  7.6361,  6.5084,  8.2657,  0.0000,\n",
      "          5.1785,  5.4031,  5.4031, 19.6995,  7.4518,  8.2657,  9.0780,  7.6361,\n",
      "          5.1785,  9.9636, 19.6995,  5.1785, 13.0915,  7.6361,  8.2657, 17.6770,\n",
      "          6.5084,  7.0416,  7.6564, 13.0915,  0.0000,  4.7916, 19.6995,  5.4031,\n",
      "         14.4932,  7.0416,  8.2657,  9.6427,  9.6427,  5.1785,  4.7916,  5.1785,\n",
      "         17.6770,  9.9636,  5.4031,  0.0000,  8.2657,  6.4264, 19.6995,  7.6564,\n",
      "          9.0780,  9.9636,  7.5544,  7.6361, 14.4932,  7.6361,  5.1785,  0.0000,\n",
      "          5.4031,  0.0000,  0.0000, 14.4932,  0.0000,  9.6427, 19.6995,  0.0000,\n",
      "          7.6361, 19.6995,  9.6427,  4.7916, 14.4932,  7.6361,  7.6361,  7.0416,\n",
      "          6.5084, 13.0915,  5.1785,  9.0780,  0.0000, 19.6995,  7.0416,  5.4031,\n",
      "          5.4031, 14.4932,  7.4518,  7.6361,  0.0000, 19.6995,  0.0000,  0.0000,\n",
      "         19.6995, 17.6770,  9.6427,  8.2657,  8.2657,  8.2657,  5.1785, 19.6995]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "DEBUG [2020-07-30 06:06:33] root:       Iterator created on file:./tmp/test_series_melt.parquet\n",
      "DEBUG [2020-07-30 06:06:37] root:   test model out @ exit tensor([[11.6680,  9.6628,  2.6094, 39.7238,  1.9797, 22.3026,  7.6564,  7.6158,\n",
      "          6.1392,  6.5289,  1.3984,  7.1237,  3.5536,  8.5500, 13.6521, 25.1061,\n",
      "          4.6287,  5.9959,  4.7509,  3.5739, 17.6971,  4.9952,  7.6361,  9.6628,\n",
      "          2.2537,  6.7545,  4.9749,  7.4518,  9.6628,  5.6075, 11.6079, 23.2037,\n",
      "         11.6079, 19.6995, 12.6706,  2.7683,  4.5880,  6.6109,  1.5744, 13.6922,\n",
      "         13.6922,  5.6075,  3.5739,  7.6361,  9.4623,  9.2811,  7.6361,  4.5880,\n",
      "         15.2140,  5.6075,  1.5744,  3.3720,  5.6075,  4.6287, 11.6079, 11.4875,\n",
      "          5.8937,  4.9952,  1.5163,  4.6287, 13.6922,  7.6564,  3.6144,  7.6361,\n",
      "          4.6287,  7.6564,  4.9749,  1.8637,  7.6564,  4.1012, 19.6795,  3.5739,\n",
      "          5.5667,  7.6564,  8.5906,  3.5536, 34.7177,  0.7557,  6.5494,  1.5744,\n",
      "          7.6564,  4.7509, 13.6922,  5.6075,  9.6026,  7.5339,  9.6026,  3.6144,\n",
      "          5.6075,  3.5536,  0.1860,  5.4031,  5.6075, 11.6881, 10.2844,  9.6828,\n",
      "         27.7092,  9.3821, 12.6105,  3.3720, 30.6728,  3.0896,  6.6314,  4.6287,\n",
      "          9.6628,  4.5880,  9.2811, 45.7311,  4.6287,  4.5880,  7.6564,  7.6361,\n",
      "         15.6946,  7.6361,  6.5494,  4.7509,  7.1647,  7.6361, 13.4920,  3.5739,\n",
      "          4.3851, 11.2068,  3.5739,  4.6287,  4.1620,  5.1989,  4.9952, 13.6922]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "do_train(dls, log, log_dir='./tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
